{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "343c4746-5090-4e01-86a4-d8d485db77d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️ Início em      : 03/05/2025 09:06:34 (Flonianópolis | SC)\n",
      "\n",
      "📋 Métricas Gerais:\n",
      "   Total originais           : 15.922\n",
      "   Duplicados removidos      : 75\n",
      "   Sem Abstract              : 0\n",
      "   Sem Affiliations          : 412\n",
      "   Sem Keywords              : 1.821\n",
      "   Total após filtragem      : 13.614\n",
      "\n",
      "=======================================================\n",
      "⏱️ Tempo de execução: 0:00:04\n",
      "📅 Finalizado em: 03/05/2025 09:06:39 (Florianópolis | SC)\n",
      "=======================================================\n",
      "→ Arquivo salvo em: /home/jovyan/Congresso UFSC2025/Portfolio_analitico.xlsx\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "filtrar_portfolio.py\n",
    "\n",
    "Script 1 para:\n",
    "  1) Ler CSV \"Portfolio_metadados.csv\";\n",
    "  2) Remover duplicados (baseado no título normalizado);\n",
    "  3) Filtrar registros sem Abstract, Affiliations ou Author Keywords;\n",
    "  4) Reordenar colunas e salvar em Excel \"Portfolio_analitico.xlsx\";\n",
    "  5) Exibir métricas de limpeza, data/hora e tempo de execução.\n",
    "\n",
    "USO NO CLUSTER UFSC (JupyterLab ou SLURM):\n",
    "  • Coloque este script e o CSV na mesma pasta;\n",
    "  • No terminal do vLab:\n",
    "      module load python/3.10 pandas openpyxl\n",
    "      python filtrar_portfolio.py\n",
    "\n",
    "IMPORTANTE (Ciência Aberta):\n",
    "  • Entrada: \"Portfolio_metadados.csv\" no mesmo diretório.\n",
    "  • Saída : \"Portfolio_analitico.xlsx\" no mesmo diretório.\n",
    "\n",
    "Autor: G.O., Renato | NEIMAC | PPGC | UFSC | Mestrado  \n",
    "Data: 01-mai-2025\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "# ─── 0) marca data/hora de início ──────────────────────────\n",
    "TZ = ZoneInfo(\"America/Sao_Paulo\")\n",
    "inicio = datetime.now(TZ)\n",
    "print(f\"▶️ Início em      : {inicio:%d/%m/%Y %H:%M:%S} (Flonianópolis | SC)\")\n",
    "\n",
    "# 1) Instalar dependências se faltarem\n",
    "def instalar(pkg):\n",
    "    try:\n",
    "        __import__(pkg)\n",
    "    except ImportError:\n",
    "        print(f\"⚙️ Instalando {pkg}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "        import site; site.addsitedir(site.USER_SITE)\n",
    "\n",
    "for pkg in (\"pandas\",\"openpyxl\"):\n",
    "    instalar(pkg)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ——————— FUNÇÕES AUXILIARES ———————\n",
    "def normalizar(texto: str) -> str:\n",
    "    if not isinstance(texto, str):\n",
    "        return \"\"\n",
    "    nfkd = unicodedata.normalize(\"NFKD\", texto)\n",
    "    s = \"\".join(c for c in nfkd if not unicodedata.combining(c)).lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def formata_numero(n, decimals=0) -> str:\n",
    "    s = f\"{n:,.{decimals}f}\"\n",
    "    return s.replace(\",\", \"X\").replace(\".\", \",\").replace(\"X\", \".\")\n",
    "\n",
    "# ——————— CONFIGURAÇÃO DE CAMINHOS ———————\n",
    "try:\n",
    "    base_dir = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    base_dir = Path.cwd()\n",
    "\n",
    "csv_path    = base_dir / \"Portfolio_metadados.csv\"\n",
    "output_path = base_dir / \"Portfolio_analitico.xlsx\"\n",
    "\n",
    "# ——————— VERIFICAÇÃO DE EXISTÊNCIA ———————\n",
    "if not csv_path.exists():\n",
    "    sys.exit(f\"⛔ Arquivo não encontrado: {csv_path}\")\n",
    "\n",
    "# ——————— LEITURA E FILTRAGEM ———————\n",
    "df = pd.read_csv(csv_path)\n",
    "total_original = len(df)\n",
    "\n",
    "# 1) Duplicados (título normalizado)\n",
    "df[\"Title_norm\"] = df[\"Title\"].apply(normalizar)\n",
    "mask_dup = df.duplicated(subset=\"Title_norm\", keep=\"first\")\n",
    "n_duplicados = int(mask_dup.sum())\n",
    "df = df[~mask_dup].reset_index(drop=True)\n",
    "\n",
    "# 2) Sem Abstract\n",
    "n_sem_abstract = int(df[\"Abstract\"].isna().sum())\n",
    "df = df.dropna(subset=[\"Abstract\"]).reset_index(drop=True)\n",
    "\n",
    "# 3) Sem Affiliations\n",
    "n_sem_affiliacao = int(df[\"Affiliations\"].isna().sum())\n",
    "df = df.dropna(subset=[\"Affiliations\"]).reset_index(drop=True)\n",
    "\n",
    "# 4) Sem Author Keywords\n",
    "n_sem_keywords = int(df[\"Author Keywords\"].isna().sum())\n",
    "df = df.dropna(subset=[\"Author Keywords\"]).reset_index(drop=True)\n",
    "\n",
    "total_filtrado = len(df)\n",
    "\n",
    "# ——————— REORDENAÇÃO DAS COLUNAS ———————\n",
    "ordem = [\n",
    "    \"Year\",\"Cited by\",\"Title\",\"Abstract\",\n",
    "    \"Affiliations\",\"Source title\",\"Author Keywords\"\n",
    "]\n",
    "df = df[ordem]\n",
    "\n",
    "# ——————— EXPORTAÇÃO PARA EXCEL ———————\n",
    "df.to_excel(output_path, index=False)\n",
    "\n",
    "# ─── FIM DA CONTAGEM DO TEMPO ───────────────────────────────\n",
    "fim = datetime.now(TZ)\n",
    "dur = fim - inicio\n",
    "dur_str = str(dur).split(\".\")[0]\n",
    "\n",
    "# ─── SAÍDA DAS MÉTRICAS ────────────────────────────────────\n",
    "print(\"\\n📋 Métricas Gerais:\")\n",
    "print(f\"   Total originais           : {formata_numero(total_original)}\")\n",
    "print(f\"   Duplicados removidos      : {formata_numero(n_duplicados)}\")\n",
    "print(f\"   Sem Abstract              : {formata_numero(n_sem_abstract)}\")\n",
    "print(f\"   Sem Affiliations          : {formata_numero(n_sem_affiliacao)}\")\n",
    "print(f\"   Sem Keywords              : {formata_numero(n_sem_keywords)}\")\n",
    "print(f\"   Total após filtragem      : {formata_numero(total_filtrado)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*55)\n",
    "print(f\"⏱️ Tempo de execução: {dur_str}\")\n",
    "print(f\"📅 Finalizado em: {fim:%d/%m/%Y %H:%M:%S} (Florianópolis | SC)\")\n",
    "print(\"=\"*55)\n",
    "print(f\"→ Arquivo salvo em: {output_path}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7c0cf61-733a-4be8-85ad-f75f3ab017c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️ Início em: 2025-05-03 09:06:59 (Florianópolis | SC)\n",
      "\n",
      "🔄 Processando palavras-chave…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13614/13614 [00:00<00:00, 32530.73linhas/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Primeiras 15 linhas do DataFrame:\n",
      "            Palavra-chave  Ocorrências  Porcentagem (%)\n",
      "      earnings management         1029             1.50\n",
      "    management accounting          791             1.16\n",
      "               accounting          790             1.16\n",
      "     corporate governance          571             0.84\n",
      "                 business          568             0.83\n",
      "management and accounting          534             0.78\n",
      "                  finance          277             0.41\n",
      "                economics          240             0.35\n",
      "           sustainability          227             0.33\n",
      "          risk management          212             0.31\n",
      "               management          211             0.31\n",
      "      financial reporting          189             0.28\n",
      "                     ifrs          173             0.25\n",
      "           accountability          161             0.24\n",
      "              performance          158             0.23\n",
      "\n",
      "🔢 Total de palavras-chave mapeadas: 68.375\n",
      "\n",
      "📊 RELATÓRIO FINAL — FREQUÊNCIA DE PALAVRAS-CHAVE\n",
      "=======================================================\n",
      "→ Total mapeado          : 68375\n",
      "⏱️ Tempo de execução: 0:00:02\n",
      "📅 Finalizado em: 03/05/2025 09:07:01 (Florianópolis | SC)\n",
      "=======================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "keyword_analitico.py\n",
    "\n",
    "Script 2 para:\n",
    "  1) Ler planilha Excel \"Portfolio_analitico.xlsx\";\n",
    "  2) Extrair e limpar as 'Author Keywords';\n",
    "  3) Contar frequência e calcular porcentagem de ocorrência;\n",
    "  4) Mostrar barra de progresso;\n",
    "  5) Gerar gráfico Top-15 em PNG;\n",
    "  6) Exportar relatório formatado em Excel (.xlsx);\n",
    "  7) Registrar data/hora de início e término da execução.\n",
    "\n",
    "USO NO CLUSTER UFSC (vLab JupyterLab ou SLURM):\n",
    "  • Coloque este script e o arquivo \"Portfolio_analitico.xlsx\" no mesmo diretório.\n",
    "  • No terminal do vLab (Launcher → Terminal), rode:\n",
    "      module load python/3.10 pandas matplotlib xlsxwriter openpyxl tqdm\n",
    "      python keyword_analitico.py\n",
    "  • OU submeta um job SLURM sem GPU:\n",
    "      #SBATCH --cpus-per-task=4\n",
    "      python keyword_analitico.py\n",
    "\n",
    "DEPENDÊNCIAS (serão instaladas automaticamente se ausentes):\n",
    "  pandas, openpyxl, matplotlib, xlsxwriter, tqdm\n",
    "\n",
    "Autor: G.O., Renato | NEIMAC | PPGC | UFSC | Mestrado  \n",
    "Data: 01-mai-2025\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "# ─── 0) marca data/hora de início (fuso São Paulo) ──────────\n",
    "SP_TZ = ZoneInfo(\"America/Sao_Paulo\")\n",
    "inicio = datetime.now(SP_TZ)\n",
    "print(f\"▶️ Início em: {inicio:%Y-%m-%d %H:%M:%S} (Florianópolis | SC)\")\n",
    "\n",
    "# 1) Instalar dependências se faltarem\n",
    "def instalar_dependencias():\n",
    "    deps = ['pandas', 'openpyxl', 'matplotlib', 'xlsxwriter', 'tqdm']\n",
    "    for pkg in deps:\n",
    "        try:\n",
    "            __import__(pkg)\n",
    "        except ImportError:\n",
    "            print(f\"⚙️ Instalando {pkg}...\")\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg])\n",
    "            import site; site.addsitedir(site.USER_SITE)\n",
    "            __import__(pkg)\n",
    "\n",
    "instalar_dependencias()\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 2) Define diretório e arquivos\n",
    "base_dir    = Path.cwd()\n",
    "input_file  = base_dir / 'Portfolio_analitico.xlsx'\n",
    "output_xlsx = base_dir / 'keyword_analitico.xlsx'\n",
    "output_png  = base_dir / 'top15_keywords.png'\n",
    "\n",
    "# 3) Verifica existência do arquivo de entrada\n",
    "if not input_file.exists():\n",
    "    sys.exit(f\"⛔ Arquivo de entrada não encontrado: {input_file}\")\n",
    "\n",
    "# 4) Leitura e validação\n",
    "df = pd.read_excel(input_file)\n",
    "if 'Author Keywords' not in df.columns:\n",
    "    sys.exit(\"⛔ Coluna 'Author Keywords' não encontrada.\")\n",
    "keywords_ser = df['Author Keywords'].dropna()\n",
    "\n",
    "# 5) Função de simplificação\n",
    "def simplify_keyword(kw: str) -> str:\n",
    "    patterns = [r'aasb\\s*\\d*', r'aaoifi\\s*', r'aaer\\s*', r'aacsb\\s*']\n",
    "    kw = kw.strip()\n",
    "    for pat in patterns:\n",
    "        kw = re.sub(pat, '', kw, flags=re.IGNORECASE).strip()\n",
    "    return kw.lower()\n",
    "\n",
    "# 6) Extração e limpeza com barra de progresso\n",
    "print(\"\\n🔄 Processando palavras-chave…\")\n",
    "irrelevant = [r'^\\d+$', r'^\\d+[a-z]$', r'century', r'audit act', r'^[a-z]{1,2}$']\n",
    "all_keywords = []\n",
    "for entry in tqdm(keywords_ser, unit='linhas'):\n",
    "    partes = re.split(r'[;,/|]+| - ', str(entry))\n",
    "    for kw in partes:\n",
    "        kw = re.sub(r'[^\\w\\s]', '', kw).strip().lower()\n",
    "        if (not kw\n",
    "            or any(re.search(p, kw) for p in irrelevant)\n",
    "            or len(kw.split()) > 3):\n",
    "            continue\n",
    "        kw = simplify_keyword(kw)\n",
    "        if not re.fullmatch(r'[a-z0-9 ]+', kw):\n",
    "            continue\n",
    "        all_keywords.append(kw)\n",
    "\n",
    "# 7) Contagem e percentual\n",
    "total = len(all_keywords)\n",
    "if total == 0:\n",
    "    sys.exit(\"⛔ Nenhuma palavra-chave válida após processamento.\")\n",
    "counts = Counter(all_keywords)\n",
    "\n",
    "# 8) Monta DataFrame de saída\n",
    "df_out = pd.DataFrame([\n",
    "    {'Palavra-chave': k, 'Ocorrências': v, 'Porcentagem (%)': round(v/total*100, 2)}\n",
    "    for k, v in counts.items()\n",
    "])\n",
    "df_out = df_out.sort_values('Ocorrências', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# 9) Saída no terminal (preview + total)\n",
    "print(\"\\n🎯 Primeiras 15 linhas do DataFrame:\")\n",
    "print(df_out.head(15).to_string(index=False))\n",
    "print(f\"\\n🔢 Total de palavras-chave mapeadas: {total:,}\".replace(',', '.'))\n",
    "\n",
    "# 10) Gráfico Top-15\n",
    "top15 = counts.most_common(15)\n",
    "if top15:\n",
    "    kws, vals = zip(*top15)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(kws, vals)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel('Ocorrências')\n",
    "    plt.title('Top 15 Palavras-Chave')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_png)\n",
    "    plt.close()\n",
    "\n",
    "# 11) Formatação e exportação Excel\n",
    "df_out['Porcentagem (%)'] = df_out['Porcentagem (%)']\\\n",
    "    .map(lambda x: f\"{x:.2f}\".replace('.', ',') + '%')\n",
    "with pd.ExcelWriter(output_xlsx, engine='xlsxwriter') as writer:\n",
    "    df_out.to_excel(writer, index=False, sheet_name='Resumo')\n",
    "    wb = writer.book\n",
    "    ws = writer.sheets['Resumo']\n",
    "    header_fmt = wb.add_format({'bold': True, 'align': 'center'})\n",
    "    text_fmt   = wb.add_format({'align': 'left'})\n",
    "    num_fmt    = wb.add_format({'num_format': '#,##0', 'align': 'right'})\n",
    "    for col_idx, _ in enumerate(df_out.columns):\n",
    "        ws.write(0, col_idx, df_out.columns[col_idx], header_fmt)\n",
    "    ws.set_column('A:A', 40, text_fmt)\n",
    "    ws.set_column('B:B', 15, num_fmt)\n",
    "    ws.set_column('C:C', 15, text_fmt)\n",
    "\n",
    "# ─── 12) Relatório final com data e duração ─────────────────\n",
    "fim     = datetime.now(SP_TZ)\n",
    "duracao = str(fim - inicio).split('.')[0]\n",
    "print(\"\\n📊 RELATÓRIO FINAL — FREQUÊNCIA DE PALAVRAS-CHAVE\")\n",
    "print(\"=\"*55)\n",
    "print(f\"→ Total mapeado          : {total}\")\n",
    "print(f\"⏱️ Tempo de execução: {duracao}\")\n",
    "print(f\"📅 Finalizado em: {fim:%d/%m/%Y %H:%M:%S} (Florianópolis | SC)\")\n",
    "print(\"=\"*55 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05d8ac82-76eb-4160-a3f7-aa37406d904a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 12:07:22,720 - INFO - pandas já instalado.\n",
      "2025-05-03 12:07:22,721 - INFO - openpyxl já instalado.\n",
      "2025-05-03 12:07:22,721 - INFO - xlsxwriter já instalado.\n",
      "2025-05-03 12:07:23,813 - INFO - torch já instalado.\n",
      "2025-05-03 12:07:23,998 - INFO - transformers já instalado.\n",
      "2025-05-03 12:07:24,368 - INFO - scikit-learn já instalado.\n",
      "2025-05-03 12:07:24,369 - INFO - tqdm já instalado.\n",
      "2025-05-03 12:07:24,369 - INFO - numpy já instalado.\n",
      "2025-05-03 12:07:24,601 - INFO - nltk já instalado.\n",
      "2025-05-03 12:07:24,602 - INFO - psutil já instalado.\n",
      "2025-05-03 12:07:24,857 - INFO - Inicializando stemmer...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️ Início em: 2025-05-03 09:07:24 (Florianópolis | SC)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Deseja ativar debug (s/n)?  s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 12:07:28,244 - DEBUG - Modo debug ativado.\n",
      "2025-05-03 12:07:28,245 - INFO - Verificando arquivo: /home/jovyan/Congresso UFSC2025/keyword_analitico.xlsx\n",
      "2025-05-03 12:07:28,245 - INFO - Carregando planilha Excel...\n",
      "2025-05-03 12:07:28,759 - INFO - Colunas encontradas: ['Palavra-chave', 'Ocorrências', 'Porcentagem (%)']\n",
      "2025-05-03 12:07:28,761 - INFO - Coluna de frequência: Ocorrências\n",
      "2025-05-03 12:07:28,762 - INFO - Usando device: cpu\n",
      "2025-05-03 12:07:28,762 - INFO - Carregando tokenizer e modelo BERT...\n",
      "2025-05-03 12:07:30.318653: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746274050.330247 2114694 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746274050.333490 2114694 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746274050.344219 2114694 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746274050.344233 2114694 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746274050.344235 2114694 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746274050.344236 2114694 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-03 12:07:30.348253: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-03 12:07:31,619 - INFO - Categorias inicializadas.\n",
      "2025-05-03 12:07:31,620 - INFO - Processando palavras-chave...\n",
      "2025-05-03 12:07:31,629 - INFO - Encontradas 26125 palavras-chave únicas.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 CATEGORIAS ENCONTRADAS NO DICIONÁRIO:\n",
      "----------------------------------------\n",
      "1. avaliação de desempenho\n",
      "2. comportamento organizacional\n",
      "3. custo\n",
      "4. educação contábil\n",
      "5. orçamento\n",
      "6. outros temas\n",
      "7. sistema de controle gerencial\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 12:07:31,900 - INFO - Calculando embeddings para 26125 textos...\n",
      "2025-05-03 12:07:31,901 - INFO - Memória disponível: 1744.27 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1661124cc632432c9c9d705a9be2e2cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando batches:   0%|          | 0/1633 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 12:08:19,881 - INFO - Embeddings calculados com sucesso.\n",
      "2025-05-03 12:08:19,962 - INFO - Calculando embeddings das categorias...\n",
      "2025-05-03 12:08:19,963 - INFO - Calculando embeddings para 7 textos...\n",
      "2025-05-03 12:08:19,964 - INFO - Memória disponível: 1749.90 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e88d6e28844179bff2eaf6093c4833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando batches:   0%|          | 0/1 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 12:08:20,082 - INFO - Embeddings calculados com sucesso.\n",
      "2025-05-03 12:08:20,083 - INFO - Classificando palavras-chave...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "385c4c46b7334985928e15abed19dedd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classificando:   0%|          | 0/26125 [00:00<?, ?kw/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 12:08:24,434 - INFO - Salvando debug...\n",
      "2025-05-03 12:08:26,384 - INFO - Debug salvo em: /home/jovyan/Congresso UFSC2025/debug_dicionario_scores.xlsx\n",
      "2025-05-03 12:08:26,386 - INFO - Gerando sugestões...\n",
      "2025-05-03 12:08:26,400 - INFO - Sugestões salvas em: /home/jovyan/Congresso UFSC2025/suggested_terms.xlsx\n",
      "2025-05-03 12:08:26,400 - INFO - Integrando sugestões...\n",
      "2025-05-03 12:08:26,401 - INFO - Calculando embeddings para 20 textos...\n",
      "2025-05-03 12:08:26,401 - INFO - Memória disponível: 1751.91 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5e39364426413ea15dcb8e4173900e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando batches:   0%|          | 0/2 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 12:08:26,551 - INFO - Embeddings calculados com sucesso.\n",
      "2025-05-03 12:08:26,556 - INFO - Atualizando descritores...\n",
      "2025-05-03 12:08:26,557 - INFO - Gerando dicionário final com duas colunas...\n",
      "2025-05-03 12:08:26,577 - INFO - Dicionário salvo em: /home/jovyan/Congresso UFSC2025/Dicionario_Contabilidade_Gerencial.xlsx\n",
      "2025-05-03 12:08:26,578 - INFO - Gerando relatório final...\n",
      "2025-05-03 12:08:26,578 - INFO - Script concluído com sucesso.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 DICIONÁRIO FINAL ATUALIZADO\n",
      "- Orçamento: 8008 keywords\n",
      "- Sistema de Controle Gerencial: 2678 keywords\n",
      "- Avaliação de desempenho: 2582 keywords\n",
      "- Outros Temas: 2125 keywords\n",
      "- Custo: 9310 keywords\n",
      "- Educação Contábil: 865 keywords\n",
      "- Não Classificado: 309 keywords\n",
      "- Comportamento Organizacional: 248 keywords\n",
      "📅 Finalizado em: 03/05/2025 09:08:26 (Florianópolis | SC)\n",
      "⏱️ Tempo de execução: 0:01:01\n",
      "→ Dicionário salvo em: Dicionario_Contabilidade_Gerencial.xlsx\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Dicionario_contabilidade_gerencial.py\n",
    "\n",
    "Script 3 para:\n",
    "  1) Carregar planilha Excel \"keyword_analitico.xlsx\" com contagem de frequência;\n",
    "  2) Classificar semanticamente cada palavra-chave em subáreas de Contabilidade Gerencial;\n",
    "  3) Ajustar automaticamente descritores de cada categoria com base nas top keywords não classificadas;\n",
    "  4) Gerar e formatar o dicionário temático em Excel \"Dicionario_Contabilidade_Gerencial.xlsx\" com duas colunas (Categoria e Palavras-chave);\n",
    "  5) Salvar scores de similaridade em \"debug_dicionario_scores.xlsx\";\n",
    "  6) Salvar sugestões de termos em \"suggested_terms.xlsx\".\n",
    "\n",
    "USO NO CLUSTER UFSC (JupyterLab ou SLURM):\n",
    "  • Instale dependências: pip install pandas openpyxl xlsxwriter torch transformers scikit-learn tqdm numpy nltk psutil\n",
    "  • No cluster: module load python/3.10 pandas torch transformers scikit-learn tqdm openpyxl xlsxwriter nltk\n",
    "  • Execute: python Dicionario_contabilidade_gerencial.py\n",
    "  • Para SLURM, use o script fornecido no final.\n",
    "\n",
    "IMPORTANTE:\n",
    "  • A planilha \"keyword_analitico.xlsx\" deve ter colunas 'Palavra-chave' e uma coluna de frequência (nome com 'freq', 'count' ou 'ocorr').\n",
    "  • Arquivo deve estar no mesmo diretório do script (/home/jovyan/Congresso UFSC2025/).\n",
    "  • A saída do dicionário será ajustada para apenas \"Categoria\" e \"Palavras-chave\" com todas as palavras classificadas.\n",
    "\n",
    "DEPENDÊNCIAS:\n",
    "  pandas, openpyxl, xlsxwriter, torch, transformers, scikit-learn, tqdm, numpy, nltk, psutil\n",
    "\n",
    "Autor: G.O., Renato | NEIMAC | PPGC | UFSC | Mestrado\n",
    "Contribuidores: [Adicione seu nome, se aplicável]\n",
    "Licença: MIT\n",
    "Data: 02/05/2025\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "import psutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Desativar TensorFlow CUDA e oneDNN\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''  # Desativa GPU\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # Desativa oneDNN\n",
    "\n",
    "# Instalar dependências\n",
    "def instalar_dependencias():\n",
    "    \"\"\"Instala as dependências necessárias se não estiverem disponíveis.\"\"\"\n",
    "    deps = ['pandas', 'openpyxl', 'xlsxwriter', 'torch', 'transformers', 'scikit-learn', 'tqdm', 'numpy', 'nltk', 'psutil']\n",
    "    for pkg in deps:\n",
    "        mod = 'sklearn' if pkg == 'scikit-learn' else pkg\n",
    "        try:\n",
    "            __import__(mod)\n",
    "            logger.info(f\"{pkg} já instalado.\")\n",
    "        except ImportError:\n",
    "            logger.info(f\"Instalando {pkg}...\")\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg])\n",
    "            import site\n",
    "            site.addsitedir(site.USER_SITE)\n",
    "\n",
    "try:\n",
    "    instalar_dependencias()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erro ao instalar dependências: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "try:\n",
    "    from tqdm.notebook import tqdm  # Para JupyterLab\n",
    "except ImportError:\n",
    "    from tqdm import tqdm  # Fallback para terminal\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Inicializar stemmer\n",
    "logger.info(\"Inicializando stemmer...\")\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# Definir caminhos e timezone\n",
    "base_dir = Path.cwd()\n",
    "input_file = base_dir / 'keyword_analitico.xlsx'\n",
    "output_file = base_dir / 'Dicionario_Contabilidade_Gerencial.xlsx'\n",
    "debug_file = base_dir / 'debug_dicionario_scores.xlsx'\n",
    "sugg_file = base_dir / 'suggested_terms.xlsx'\n",
    "\n",
    "SC_TZ = ZoneInfo('America/Sao_Paulo')\n",
    "inicio = datetime.now(SC_TZ)\n",
    "\n",
    "# Exibir início\n",
    "print(f\"▶️ Início em: {inicio:%Y-%m-%d %H:%M:%S} (Florianópolis | SC)\")\n",
    "\n",
    "# Perguntar sobre debug\n",
    "debug_mode = input(\"Deseja ativar debug (s/n)? \").strip().lower() == 's'\n",
    "if debug_mode:\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    logger.debug(\"Modo debug ativado.\")\n",
    "\n",
    "# Verificar arquivo de entrada\n",
    "logger.info(f\"Verificando arquivo: {input_file}\")\n",
    "if not input_file.exists():\n",
    "    logger.error(f\"Arquivo {input_file} não encontrado.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Carregar planilha\n",
    "logger.info(\"Carregando planilha Excel...\")\n",
    "try:\n",
    "    df = pd.read_excel(input_file)\n",
    "    logger.info(f\"Colunas encontradas: {list(df.columns)}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erro ao carregar planilha: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "freq_cols = [c for c in df.columns if any(x in c.lower() for x in ['freq', 'count', 'ocorr'])]\n",
    "if 'Palavra-chave' not in df.columns or not freq_cols:\n",
    "    logger.error(\"A planilha deve ter colunas 'Palavra-chave' e frequência (com 'freq', 'count' ou 'ocorr').\")\n",
    "    sys.exit(1)\n",
    "freq_col = freq_cols[0]\n",
    "logger.info(f\"Coluna de frequência: {freq_col}\")\n",
    "\n",
    "# Configurar dispositivo e modelo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Usando device: {device}\")\n",
    "try:\n",
    "    logger.info(\"Carregando tokenizer e modelo BERT...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "    model = AutoModel.from_pretrained('bert-base-multilingual-cased').to(device)\n",
    "    model.eval()\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erro ao carregar modelo BERT: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Definir descritores das categorias (atualizado com as palavras sugeridas)\n",
    "categorias = {\n",
    "    'Avaliação de desempenho': (\n",
    "        'performance management; performance evaluation; performance appraisal; performance measurement;'\n",
    "        'kpi; roi; roa; bsc; balanced scorecard; benchmarking; efficiency analysis; value-based performance;'\n",
    "        'economic value added; eva; employee performance; performance review; productivity metrics;'\n",
    "        'performance feedback; goal setting; performance indicators; outcome measurement; performance tracking;'\n",
    "        'performance standards; employee appraisal; performance assessment; organizational metrics;'\n",
    "        'performance improvement'\n",
    "    ),\n",
    "    'Orçamento': (\n",
    "        'budgeting; budget planning; budget forecast; budget participation; budgetary control; beyond budgeting;'\n",
    "        'rolling forecast; zero-based budgeting; incremental budgeting; budget constraints; budget approval;'\n",
    "        'budget cycle; budget preparation; budget allocation; budget review; budget oversight'\n",
    "    ),\n",
    "    'Custo': (\n",
    "        'cost accounting; cost systems; abc costing; activity-based costing; absorption costing;'\n",
    "        'variable costing; target costing; kaizen costing; standard costing; life cycle costing;'\n",
    "        'cost control; cost analysis; cost estimate; cost drivers; cost behavior; '\n",
    "        'direct costs; indirect costs; cost structure; unit cost'\n",
    "    ),\n",
    "    'Sistema de Controle Gerencial': (\n",
    "        'management control systems; mcs; control systems; levers of control; belief systems;'\n",
    "        'boundary systems; strategic control; operational control; internal control systems;'\n",
    "        'accounting information systems; results control; action control; diagnostic control;'\n",
    "        'interactive control; control mechanisms; control practices; control frameworks;'\n",
    "        'management accounting systems; control policies; control feedback mechanisms; control processes;'\n",
    "        'management oversight; control implementation'\n",
    "    ),\n",
    "    'Comportamento Organizacional': (\n",
    "        'organizational behavior; employee engagement; motivation theories; cognitive bias;'\n",
    "        'organizational culture; leadership styles; team dynamics; organizational commitment;'\n",
    "        'psychological safety; work motivation; workplace behavior; job satisfaction;'\n",
    "        'organizational justice; organizational psychology; affective commitment; job involvement;'\n",
    "        'turnover intention; group behavior; interpersonal conflict; communication styles;'\n",
    "        'power dynamics; organizational learning; diversity management; emotional intelligence;'\n",
    "        'behavioral decision-making; team performance; workplace dynamics; employee motivation;'\n",
    "        'organizational change; conflict resolution'\n",
    "    ),\n",
    "    'Educação Contábil': (\n",
    "        'accounting education; curriculum development; teaching methods; learning strategies;'\n",
    "        'pedagogy in higher education; professional development; continuing education; instructional design;'\n",
    "        'accounting instructor; accounting teaching; educational innovation; didactic strategies;'\n",
    "        'blended learning; teaching accounting; student academic performance; learning outcomes; capstone course;'\n",
    "        'ethics education; teaching cases; IFRS education; audit education; tax education; critical thinking;'\n",
    "        'student assessment; accounting skills; competency-based education; student engagement;'\n",
    "        'accounting students; accounting pedagogy; accounting curriculum; student learning;'\n",
    "        'accounting certification; accounting faculty; accounting courses; student accounting skills;'\n",
    "        'accounting departments; academic accounting; accounting academics; accounting research education'\n",
    "    ),\n",
    "    'Outros Temas': (\n",
    "        'general business trends; non-specific industry topics; miscellaneous organizational concepts;'\n",
    "        'emerging technologies; social impact initiatives; global economic trends; cross-industry innovation;'\n",
    "        'non-accounting academic research; societal trends; cultural studies; public administration;'\n",
    "        'nonprofit management; social entrepreneurship; community development; human rights; gender equality;'\n",
    "        'environmental policy; urban development; smart cities; systemic change'\n",
    "    )\n",
    "}\n",
    "\n",
    "# Exibir categorias encontradas\n",
    "print(\"📋 CATEGORIAS ENCONTRADAS NO DICIONÁRIO:\")\n",
    "print(\"----------------------------------------\")\n",
    "for i, cat in enumerate(sorted(categorias.keys()), 1):\n",
    "    print(f\"{i}. {cat.lower()}\")\n",
    "print(\"----------------------------------------\")\n",
    "logger.info(\"Categorias inicializadas.\")\n",
    "\n",
    "# Função de embedding\n",
    "def embed_texts(texts, batch_size=16):\n",
    "    \"\"\"Gera embeddings para uma lista de textos usando BERT.\"\"\"\n",
    "    logger.info(f\"Calculando embeddings para {len(texts)} textos...\")\n",
    "    mem = psutil.virtual_memory()\n",
    "    logger.info(f\"Memória disponível: {mem.available / (1024**3):.2f} GB\")\n",
    "    if mem.available < 1 * (1024**3):\n",
    "        logger.warning(\"Memória baixa! Pode causar travamento.\")\n",
    "    embs = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Processando batches\", unit=\"batch\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        try:\n",
    "            enc = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "            with torch.no_grad():\n",
    "                out = model(**enc).last_hidden_state\n",
    "            embs.append(out.mean(1).cpu().numpy())\n",
    "            torch.cuda.empty_cache() if device.type == 'cuda' else None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erro no batch {i//batch_size + 1}: {e}\")\n",
    "            raise\n",
    "    logger.info(\"Embeddings calculados com sucesso.\")\n",
    "    return np.vstack(embs)\n",
    "\n",
    "# Processar palavras-chave\n",
    "logger.info(\"Processando palavras-chave...\")\n",
    "unique_kws = sorted(df['Palavra-chave'].astype(str).unique())\n",
    "logger.info(f\"Encontradas {len(unique_kws)} palavras-chave únicas.\")\n",
    "stems = [' '.join(stemmer.stem(tok) for tok in re.findall(r\"\\w+\", kw.lower())) for kw in unique_kws]\n",
    "try:\n",
    "    kw_emb = embed_texts(stems)\n",
    "    kw_emb /= np.linalg.norm(kw_emb, axis=1, keepdims=True)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erro ao calcular embeddings das palavras-chave: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Embeddings das categorias\n",
    "logger.info(\"Calculando embeddings das categorias...\")\n",
    "descs = list(categorias.values())\n",
    "cat_keys = list(categorias.keys())\n",
    "try:\n",
    "    cat_emb = embed_texts(descs)\n",
    "    cat_emb /= np.linalg.norm(cat_emb, axis=1, keepdims=True)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erro ao calcular embeddings das categorias: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Regras manuais de reclassificação baseadas na análise\n",
    "reclass_rules = {\n",
    "    '6g cellular communication': 'Outros Temas',\n",
    "    '1c company': 'Sistema de Controle Gerencial',\n",
    "    '401k investment': 'Orçamento',\n",
    "    '2003 northeast blackout': 'Outros Temas',\n",
    "    '10k report': 'Avaliação de desempenho',\n",
    "    'a76 studies': 'Outros Temas',\n",
    "    'team productivity': 'Comportamento Organizacional',\n",
    "    'planning and budgeting': 'Orçamento',\n",
    "    'audit committee': 'Outros Temas',\n",
    "    'digital technologies': 'Outros Temas',\n",
    "    'learning curve': 'Educação Contábil',\n",
    "}\n",
    "\n",
    "# Classificar palavras-chave\n",
    "logger.info(\"Classificando palavras-chave...\")\n",
    "debug = []\n",
    "cats_assigned = []\n",
    "for idx, emb in enumerate(tqdm(kw_emb, desc='Classificando', unit='kw')):\n",
    "    kw = unique_kws[idx]\n",
    "    if kw in reclass_rules:\n",
    "        cat = reclass_rules[kw]\n",
    "    else:\n",
    "        sims = cosine_similarity([emb], cat_emb)[0]\n",
    "        imax = sims.argmax()\n",
    "        cat = cat_keys[imax] if sims[imax] >= 0.3 else 'Não Classificado'  # Mantido limiar em 0.3\n",
    "    cats_assigned.append(cat)\n",
    "    debug.append({'keyword': kw, **{cat_keys[i]: float(sims[i]) for i in range(len(cat_keys))}, 'assigned': cat})\n",
    "\n",
    "# Contar palavras por categoria\n",
    "word_counts = Counter(cats_assigned)\n",
    "\n",
    "# Salvar debug\n",
    "logger.info(\"Salvando debug...\")\n",
    "try:\n",
    "    debug_df = pd.DataFrame(debug)\n",
    "    debug_df.to_excel(debug_file, index=False)\n",
    "    logger.info(f\"Debug salvo em: {debug_file}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erro ao salvar debug: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Sugestões de termos\n",
    "logger.info(\"Gerando sugestões...\")\n",
    "nao_cl = debug_df[debug_df.assigned == 'Não Classificado'][['keyword']].drop_duplicates()\n",
    "freq_df = df[['Palavra-chave', freq_col]].rename(columns={'Palavra-chave': 'keyword', freq_col: 'frequency'})\n",
    "sugg = nao_cl.merge(freq_df, on='keyword', how='left').sort_values('frequency', ascending=False).head(20)\n",
    "try:\n",
    "    sugg.to_excel(sugg_file, index=False)\n",
    "    logger.info(f\"Sugestões salvas em: {sugg_file}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erro ao salvar sugestões: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Integrar sugestões\n",
    "logger.info(\"Integrando sugestões...\")\n",
    "new_terms = defaultdict(list)\n",
    "sugg_stems = [' '.join(stemmer.stem(tok) for tok in re.findall(r\"\\w+\", kw.lower())) for kw in sugg.keyword]\n",
    "try:\n",
    "    sugg_emb = embed_texts(sugg_stems)\n",
    "    sugg_emb /= np.linalg.norm(sugg_emb, axis=1, keepdims=True)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erro ao calcular embeddings das sugestões: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "for kw, emb in zip(sugg.keyword, sugg_emb):\n",
    "    sims = cosine_similarity([emb], cat_emb)[0]\n",
    "    imax = sims.argmax()\n",
    "    cat = cat_keys[imax] if sims[imax] >= 0.3 else None  # Mantido limiar em 0.3\n",
    "    if cat:\n",
    "        new_terms[cat].append(kw)\n",
    "\n",
    "# Atualizar descritores\n",
    "logger.info(\"Atualizando descritores...\")\n",
    "for cat, terms in new_terms.items():\n",
    "    if terms:\n",
    "        categorias[cat] += '; ' + '; '.join(terms)\n",
    "\n",
    "# Gerar dicionário com duas colunas\n",
    "logger.info(\"Gerando dicionário final com duas colunas...\")\n",
    "try:\n",
    "    # Criar dicionário com palavras classificadas\n",
    "    cat_words = defaultdict(list)\n",
    "    for kw, cat in zip(unique_kws, cats_assigned):\n",
    "        cat_words[cat].append(kw)\n",
    "    dict_df = pd.DataFrame([\n",
    "        (cat, '; '.join(sorted(words))) for cat, words in cat_words.items()\n",
    "    ], columns=['Categoria', 'Palavras-chave'])\n",
    "    dict_df = dict_df.sort_values('Categoria')\n",
    "    \n",
    "    # Salvar no Excel\n",
    "    with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:\n",
    "        dict_df.to_excel(writer, index=False, sheet_name='Dicionário')\n",
    "        wb = writer.book\n",
    "        ws = writer.sheets['Dicionário']\n",
    "        fmt = wb.add_format({'font_name': 'Times New Roman', 'font_size': 12, 'align': 'center', 'valign': 'vcenter'})\n",
    "        ws.set_column('A:A', 25, fmt)\n",
    "        ws.set_column('B:B', 80, fmt)\n",
    "    logger.info(f\"Dicionário salvo em: {output_file}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erro ao gerar dicionário: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Relatório final\n",
    "logger.info(\"Gerando relatório final...\")\n",
    "fim = datetime.now(SC_TZ)\n",
    "dur = str(fim - inicio).split('.')[0]\n",
    "\n",
    "print(\"📊 DICIONÁRIO FINAL ATUALIZADO\")\n",
    "for c, n in word_counts.items():\n",
    "    print(f\"- {c}: {n} keywords\")\n",
    "print(f\"📅 Finalizado em: {fim:%d/%m/%Y %H:%M:%S} (Florianópolis | SC)\")\n",
    "print(f\"⏱️ Tempo de execução: {dur}\")\n",
    "print(f\"→ Dicionário salvo em: {output_file.name}\")\n",
    "logger.info(\"Script concluído com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "401648ec-0bef-43ea-afb8-d1bd4d406186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️ Início em: 2025-05-03 09:10:07 (Florianópolis | SC)\n",
      "Baixando recursos do NLTK...\n",
      "Recursos do NLTK baixados com sucesso.\n",
      "Carregando /home/jovyan/Congresso UFSC2025/Dicionario_Contabilidade_Gerencial.xlsx...\n",
      "Dicionário carregado com 8 categorias.\n",
      "Carregando /home/jovyan/Congresso UFSC2025/SJR 2023.xls...\n",
      "SJR 2023 carregado com 1576 entradas.\n",
      "Carregando /home/jovyan/Congresso UFSC2025/Portfolio_analitico.xlsx...\n",
      "Planilha de artigos carregada com 13614 linhas.\n",
      "Extraindo países da coluna 'Affiliations'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13614/13614 [00:00<00:00, 794573.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associando 'Best Quartile' com base no SJR 2023...\n",
      "Classificando artigos por temática...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13614/13614 [00:06<00:00, 2029.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvando resultado em /home/jovyan/Congresso UFSC2025/Portfolio_analitico_classificado.xlsx...\n",
      "\n",
      "Resumo de artigos por categoria:\n",
      "               Grupo Temático  Contagem\n",
      "                    Orçamento      3485\n",
      "             Não Classificado      2691\n",
      "                        Custo      2025\n",
      " Comportamento Organizacional      1364\n",
      "                 Outros Temas      1268\n",
      "Sistema de Controle Gerencial      1258\n",
      "      Avaliação de desempenho       933\n",
      "            Educação Contábil       590\n",
      "\n",
      "📅 Finalizado em: 03/05/2025 09:10:18 (Florianópolis | SC)\n",
      "⏱️ Tempo de execução: 0h 0m 10s\n",
      "→ Classificação salva em: Portfolio_analitico_classificado.xlsx\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "classificar_artigos_tematica.py (Versão Ajustada Final)\n",
    "\n",
    "Script 4 para:\n",
    "  1) Normalização dos scores por número de descritores por categoria.\n",
    "  2) Adição de n-gramas para capturar expressões compostas.\n",
    "  3) Pré-processamento com remoção de stop words, pontuação e normalização.\n",
    "  4) Otimização com índice invertido.\n",
    "  5) Inclusão de debug para análise dos scores.\n",
    "\n",
    "USO NO CLUSTER UFSC (JupyterLab ou SLURM):\n",
    "  • Instale dependências: pip install pandas openpyxl nltk tqdm\n",
    "  • No cluster: module load python/3.10 pandas openpyxl nltk\n",
    "  • Execute: python classificar_artigos_tematica.py\n",
    "  • Arquivos devem estar no mesmo diretório.\n",
    "\n",
    "IMPORTANTE:\n",
    "  • \"Portfolio_analitico.xlsx\" deve ter as colunas 'Year', 'Cited by', 'Title', 'Abstract', 'Affiliations', 'Source title', 'Author Keywords'.\n",
    "  • \"Dicionario_Contabilidade_Gerencial.xlsx\" e \"SJR 2023.xlsx\" devem estar no diretório atual.\n",
    "  • Saída será salva como \"Portfolio_analitico_classificado.xlsx\" com as abas 'Classificação' e 'Resumo_Categorias'.\n",
    "\n",
    "DEPENDÊNCIAS:\n",
    "  pandas, openpyxl, nltk, tqdm\n",
    "\n",
    "Autor: G.O., Renato | NEIMAC | PPGC | UFSC | Mestrado\n",
    "Revisões: Grok (xAI)\n",
    "Data: 02/05/2025\n",
    "Licença: MIT (Ciência Aberta - Open Science Framework)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Definir timezone e marcar início\n",
    "SC_TZ = ZoneInfo('America/Sao_Paulo')\n",
    "inicio = datetime.now(SC_TZ)\n",
    "\n",
    "print(f\"▶️ Início em: {inicio:%Y-%m-%d %H:%M:%S} (Florianópolis | SC)\")\n",
    "\n",
    "# Inicializar NLTK e baixar os recursos necessários\n",
    "print(\"Baixando recursos do NLTK...\")\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "print(\"Recursos do NLTK baixados com sucesso.\")\n",
    "\n",
    "# Habilitar progresso com pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Definir caminhos\n",
    "base_dir = Path.cwd()\n",
    "input_dict_file = base_dir / 'Dicionario_Contabilidade_Gerencial.xlsx'\n",
    "input_sjr_file = base_dir / 'SJR 2023.xls'\n",
    "input_artigos_file = base_dir / 'Portfolio_analitico.xlsx'\n",
    "output_file = base_dir / 'Portfolio_analitico_classificado.xlsx'\n",
    "\n",
    "# Carregar o dicionário temático\n",
    "print(f\"Carregando {input_dict_file}...\")\n",
    "try:\n",
    "    dict_df = pd.read_excel(input_dict_file)\n",
    "    categorias = dict_df.set_index('Categoria')['Palavras-chave'].to_dict()\n",
    "    print(f\"Dicionário carregado com {len(categorias)} categorias.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Erro: Arquivo {input_dict_file} não encontrado.\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar o dicionário: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Construir índice invertido para otimização\n",
    "def build_inverted_index(categorias):\n",
    "    index = {}\n",
    "    palavras_por_categoria = {}\n",
    "    for categoria, palavras in categorias.items():\n",
    "        palavras_lista = re.split(r'[,;]\\s*', palavras.lower())\n",
    "        palavras_por_categoria[categoria] = len(palavras_lista)\n",
    "        for palavra in palavras_lista:\n",
    "            if palavra not in index:\n",
    "                index[palavra] = []\n",
    "            index[palavra].append(categoria)\n",
    "    return index, palavras_por_categoria\n",
    "\n",
    "inverted_index, palavras_por_categoria = build_inverted_index(categorias)\n",
    "\n",
    "# Carregar o arquivo SJR 2023\n",
    "print(f\"Carregando {input_sjr_file}...\")\n",
    "try:\n",
    "    sjr_df = pd.read_excel(input_sjr_file)\n",
    "    if not all(col in sjr_df.columns for col in ['Title', 'Best Quartile']):\n",
    "        print(\"Erro: A planilha 'SJR 2023.xls' deve ter as colunas 'Title' e 'Best Quartile'.\")\n",
    "        exit(1)\n",
    "    sjr_dict = pd.Series(\n",
    "        sjr_df['Best Quartile'].values,\n",
    "        index=sjr_df['Title'].str.lower().str.strip()\n",
    "    ).to_dict()\n",
    "    print(f\"SJR 2023 carregado com {len(sjr_dict)} entradas.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Erro: Arquivo {input_sjr_file} não encontrado.\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar o arquivo SJR: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Carregar a planilha de artigos\n",
    "print(f\"Carregando {input_artigos_file}...\")\n",
    "try:\n",
    "    artigos_df = pd.read_excel(input_artigos_file)\n",
    "    expected_cols = ['Year', 'Cited by', 'Title', 'Abstract', 'Affiliations', 'Source title', 'Author Keywords']\n",
    "    if not all(col in artigos_df.columns for col in expected_cols):\n",
    "        print(f\"Erro: A planilha 'Portfolio_analitico.xlsx' deve ter as colunas {expected_cols}.\")\n",
    "        print(f\"Colunas disponíveis: {artigos_df.columns.tolist()}\")\n",
    "        exit(1)\n",
    "    print(f\"Planilha de artigos carregada com {len(artigos_df)} linhas.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Erro: Arquivo {input_artigos_file} não encontrado.\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar os artigos: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Funções utilitárias\n",
    "def extrair_pais(affiliations):\n",
    "    if pd.isna(affiliations) or affiliations == '':\n",
    "        return ''\n",
    "    aff_text = str(affiliations).split(';')[0]\n",
    "    if ',' in aff_text:\n",
    "        pais = aff_text.split(',')[-1].strip()\n",
    "        return pais if pais else 'Não Identificado'\n",
    "    return 'Não Identificado'\n",
    "\n",
    "# Pré-processamento de texto\n",
    "stop_words = set(stopwords.words('english') + stopwords.words('portuguese'))\n",
    "def preprocess_text(texto):\n",
    "    texto = str(texto).lower()\n",
    "    texto = re.sub(r'[^\\w\\s]', '', texto)  # Remover pontuação\n",
    "    texto_tokens = word_tokenize(texto)\n",
    "    texto_tokens = [t for t in texto_tokens if t not in stop_words]\n",
    "    # Adicionar bigramas\n",
    "    bigrams = [' '.join(ng) for ng in ngrams(texto_tokens, 2)]\n",
    "    texto_tokens.extend(bigrams)\n",
    "    return texto_tokens\n",
    "\n",
    "# Função de classificação com debug\n",
    "def classificar_artigo(texto):\n",
    "    texto_tokens = preprocess_text(texto)\n",
    "    categoria_counts = {}\n",
    "    for token in texto_tokens:\n",
    "        if token in inverted_index:\n",
    "            for categoria in inverted_index[token]:\n",
    "                categoria_counts[categoria] = categoria_counts.get(categoria, 0) + 1\n",
    "    scores = {}\n",
    "    melhor_categoria = None\n",
    "    max_taxa = 0\n",
    "    min_correspondencias = 2  # Limiar mínimo\n",
    "    for categoria, count in categoria_counts.items():\n",
    "        if count >= min_correspondencias:\n",
    "            taxa = count / palavras_por_categoria[categoria]\n",
    "            scores[categoria] = taxa\n",
    "            if taxa > max_taxa:\n",
    "                max_taxa = taxa\n",
    "                melhor_categoria = categoria\n",
    "        else:\n",
    "            scores[categoria] = 0\n",
    "    return melhor_categoria if melhor_categoria else 'Não Classificado', scores\n",
    "\n",
    "# Processamento das colunas\n",
    "print(\"Extraindo países da coluna 'Affiliations'...\")\n",
    "artigos_df['País'] = artigos_df['Affiliations'].progress_apply(extrair_pais)\n",
    "\n",
    "print(\"Associando 'Best Quartile' com base no SJR 2023...\")\n",
    "artigos_df['Best Quartile'] = (\n",
    "    artigos_df['Source title']\n",
    "    .str.lower().str.strip()\n",
    "    .map(sjr_dict)\n",
    "    .fillna('Não Encontrado')\n",
    ")\n",
    "\n",
    "print(\"Classificando artigos por temática...\")\n",
    "resultados = artigos_df.progress_apply(\n",
    "    lambda row: classificar_artigo(\n",
    "        ' '.join([str(row['Title']), str(row['Abstract']), str(row['Author Keywords'])])\n",
    "    ), axis=1\n",
    ")\n",
    "artigos_df['Grupo Temático'] = [res[0] for res in resultados]\n",
    "artigos_df['Debug_Scores'] = [str(res[1]) for res in resultados]\n",
    "\n",
    "# Reorganizar colunas\n",
    "final_columns = ['Year', 'Cited by', 'Title', 'Abstract', 'Affiliations', 'País',\n",
    "                 'Source title', 'Best Quartile', 'Author Keywords', 'Grupo Temático', 'Debug_Scores']\n",
    "artigos_df = artigos_df[final_columns]\n",
    "\n",
    "# Salvar resultados\n",
    "print(f\"Salvando resultado em {output_file}...\")\n",
    "try:\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        artigos_df.to_excel(writer, index=False, sheet_name='Classificação')\n",
    "        resumo_df = (\n",
    "            artigos_df['Grupo Temático']\n",
    "            .value_counts()\n",
    "            .rename_axis('Grupo Temático')\n",
    "            .reset_index(name='Contagem')\n",
    "        )\n",
    "        resumo_df.to_excel(writer, index=False, sheet_name='Resumo_Categorias')\n",
    "    print(\"\\nResumo de artigos por categoria:\")\n",
    "    print(resumo_df.to_string(index=False))\n",
    "\n",
    "    fim = datetime.now(SC_TZ)\n",
    "    dur = fim - inicio\n",
    "    horas, resto = divmod(dur.seconds, 3600)\n",
    "    minutos, segundos = divmod(resto, 60)\n",
    "    print(f\"\\n📅 Finalizado em: {fim:%d/%m/%Y %H:%M:%S} (Florianópolis | SC)\")\n",
    "    print(f\"⏱️ Tempo de execução: {horas}h {minutos}m {segundos}s\")\n",
    "    print(f\"→ Classificação salva em: {output_file.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao salvar a classificação: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b3b630b-39c8-4164-90b0-3ba1e6860ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️ Início em: 2025-05-03 09:12:56 (Florianópolis | SC)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 12:12:59,748 - INFO - Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️ Usando device: cpu\n",
      "\n",
      "⏳ Gerando embeddings dos títulos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d3b3b91ddb4c3bb4e3185018acff6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Gerando embeddings dos resumos...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ca28352b414ee296b008eb59216686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/213 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ Calculando similaridades título vs. resumo...\n",
      "\n",
      "===== RELATÓRIO FINAL =====\n",
      "Total analisados       : 13.614\n",
      "Alta Coerência         : 1.179 (8.7%)\n",
      "Média Coerência        : 8.573 (63.0%)\n",
      "Baixa Coerência        : 3.862 (28.4%)\n",
      "Sem dados/Erro         : 0 (0.0%)\n",
      "Tempo de execução: 0:01:37\n",
      "Finalizado em: 2025-05-03 09:14:33 (Florianópolis | SC)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "coerencia_comunicacional.py\n",
    "\n",
    "Script 5 para:\n",
    "  1) Ler Portfolio_analitico_classificado.xlsx e preservar dados & formatação da coluna J;\n",
    "  2) Calcular score de coerência título vs. resumo com SBERT em batch;\n",
    "  3) Gerar histogramas e boxplot;\n",
    "  4) Destacar 1% de piores coerências nas colunas C e D;\n",
    "  5) Restaurar coluna J, formatar planilha e salvar em Portfolio_analitico_coerencia.xlsx;\n",
    "  6) Exibir relatório final com contagens, tempo e data de execução.\n",
    "\n",
    "USO NO CLUSTER UFSC (JupyterLab ou SLURM):\n",
    "  • Carregue GPU e Python 3.10:\n",
    "      module load cuda/11.7 python/3.10\n",
    "  • Execute:\n",
    "      python coerencia_comunicacional.py\n",
    "\n",
    "DEPENDÊNCIAS (serão instaladas automaticamente):\n",
    "  sentence-transformers, pandas, openpyxl, tqdm, matplotlib\n",
    "\n",
    "Autor: G.O., Renato | NEIMAC | PPGC | UFSC | Mestrado\n",
    "Data: 02/05/2025\n",
    "\"\"\"\n",
    "\n",
    "import sys, subprocess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import PatternFill, Font, Alignment\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 0) Instala deps se necessário\n",
    "def instalar(pkg):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "for pkg in (\"sentence-transformers\",\"pandas\",\"openpyxl\",\"tqdm\",\"matplotlib\"):\n",
    "    try:\n",
    "        __import__(pkg.replace('-', '_'))\n",
    "    except ImportError:\n",
    "        print(f\"⚙️ Instalando {pkg}...\")\n",
    "        instalar(pkg)\n",
    "\n",
    "# --- INÍCIO (fuso São Paulo) -------------------------------------\n",
    "SP_TZ       = ZoneInfo(\"America/Sao_Paulo\")\n",
    "data_inicio = datetime.now(SP_TZ)\n",
    "print(f\"▶️ Início em: {data_inicio:%Y-%m-%d %H:%M:%S} (Florianópolis | SC)\\n\")\n",
    "\n",
    "# 1) Caminhos de entrada e saída\n",
    "def get_paths():\n",
    "    try:\n",
    "        base = Path(__file__).resolve().parent\n",
    "    except NameError:\n",
    "        base = Path.cwd()\n",
    "    inp = base / \"Portfolio_analitico_classificado.xlsx\"\n",
    "    out = base / \"Portfolio_analitico_coerencia.xlsx\"\n",
    "    if not inp.exists():\n",
    "        sys.exit(f\"⛔ Arquivo não encontrado: {inp}\")\n",
    "    return inp, out\n",
    "EXCEL_IN, EXCEL_OUT = get_paths()\n",
    "\n",
    "# 2) Carrega DataFrame e preserva coluna J\n",
    "def carregar_planilha(path):\n",
    "    df = pd.read_excel(path)\n",
    "    wb = load_workbook(path)\n",
    "    ws = wb.active\n",
    "    backup_j = {}\n",
    "    for row in range(2, ws.max_row+1):\n",
    "        cell = ws[f\"J{row}\"]\n",
    "        backup_j[row] = {'value': cell.value, 'fill': cell.fill.copy() if cell.fill and cell.fill.fill_type else None}\n",
    "    wb.close()\n",
    "    return df, backup_j\n",
    "\n",
    "# 3) Calcular coerência corretamente usando título vs. abstract\n",
    "def calcular_coerencia(df):\n",
    "    device = 'cuda' if util.torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"▶️ Usando device: {device}\\n\")\n",
    "    model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', device=device)\n",
    "    titles    = df['Title'].fillna('').astype(str).tolist()\n",
    "    abstracts = df['Abstract'].fillna('').astype(str).tolist()\n",
    "    print(\"⏳ Gerando embeddings dos títulos...\")\n",
    "    emb_titles = model.encode(titles, batch_size=64, show_progress_bar=True, convert_to_tensor=True)\n",
    "    print(\"⏳ Gerando embeddings dos resumos...\")\n",
    "    emb_abstracts = model.encode(abstracts, batch_size=64, show_progress_bar=True, convert_to_tensor=True)\n",
    "    print(\"⚡ Calculando similaridades título vs. resumo...\")\n",
    "    cosines = util.cos_sim(emb_titles, emb_abstracts).diag().cpu().numpy()\n",
    "    faixas = []\n",
    "    for s in cosines:\n",
    "        if s >= 0.80:   faixas.append('Alta')\n",
    "        elif s >= 0.60: faixas.append('Média')\n",
    "        else:           faixas.append('Baixa')\n",
    "    df['Coerência (Score)']          = cosines\n",
    "    df['Classificação de Coerência'] = faixas\n",
    "    return df, cosines, faixas\n",
    "\n",
    "# 4) Gera e salva plots\n",
    "def plot_distribuicoes(scores):\n",
    "    valid = [s for s in scores if not np.isnan(s)]\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.hist(valid, bins=30, edgecolor='black')\n",
    "    plt.title('Distribuição dos Scores de Coerência')\n",
    "    plt.xlabel('Score de Similaridade')\n",
    "    plt.ylabel('Frequência')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Histograma_Coerencia.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.boxplot(valid, vert=False)\n",
    "    plt.title('Boxplot dos Scores de Coerência')\n",
    "    plt.xlabel('Score de Similaridade')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Boxplot_Coerencia.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# 5) Salva novo Excel com backup coluna J\n",
    "def salvar_com_backup(df, backup):\n",
    "    df.to_excel(EXCEL_OUT, index=False)\n",
    "    wb = load_workbook(EXCEL_OUT)\n",
    "    ws = wb.active\n",
    "    for row, info in backup.items():\n",
    "        cell = ws[f\"J{row}\"]\n",
    "        cell.value = info['value']\n",
    "        if info['fill']:\n",
    "            cell.fill = info['fill']\n",
    "    return wb, ws\n",
    "\n",
    "# 6) Destacar 1% piores coerências\n",
    "def destacar_menores(df, ws):\n",
    "    valid = df[df['Coerência (Score)'].notnull()]\n",
    "    n = max(1, int(0.01 * len(valid)))\n",
    "    worst = valid.nsmallest(n, 'Coerência (Score)').index.tolist()\n",
    "    fill = PatternFill(start_color='FFFF00', end_color='FFFF00', fill_type='solid')\n",
    "    for idx in worst:\n",
    "        row = idx + 2\n",
    "        ws[f\"C{row}\"].fill = fill\n",
    "        ws[f\"D{row}\"].fill = fill\n",
    "\n",
    "# 7) Formatação geral\n",
    "def formatar(ws):\n",
    "    for row in ws.iter_rows(min_row=1, max_row=ws.max_row,\n",
    "                             min_col=1, max_col=ws.max_column):\n",
    "        for c in row:\n",
    "            c.font = Font(name='Calibri', size=11)\n",
    "            c.alignment = Alignment(horizontal='center', vertical='center')\n",
    "    for c in ws[1]: c.font = Font(bold=True)\n",
    "\n",
    "# 8) Relatório final\n",
    "def relatorio_final(df, faixas, start):\n",
    "    total = len(df)\n",
    "    cnt = {k: faixas.count(k) for k in ['Alta','Média','Baixa']}\n",
    "    none = total - sum(cnt.values())\n",
    "    now  = datetime.now(SP_TZ)\n",
    "    dur  = str(now - start).split('.')[0]\n",
    "    fmt = lambda x: f\"{x:,d}\".replace(',', '.')\n",
    "    print(\"\\n===== RELATÓRIO FINAL =====\")\n",
    "    print(f\"Total analisados       : {fmt(total)}\")\n",
    "    print(f\"Alta Coerência         : {fmt(cnt['Alta'])} ({cnt['Alta']/total*100:.1f}%)\")\n",
    "    print(f\"Média Coerência        : {fmt(cnt['Média'])} ({cnt['Média']/total*100:.1f}%)\")\n",
    "    print(f\"Baixa Coerência        : {fmt(cnt['Baixa'])} ({cnt['Baixa']/total*100:.1f}%)\")\n",
    "    print(f\"Sem dados/Erro         : {fmt(none)} ({none/total*100:.1f}%)\")\n",
    "    print(f\"Tempo de execução: {dur}\")\n",
    "    print(f\"Finalizado em: {now:%Y-%m-%d %H:%M:%S} (Florianópolis | SC)\")\n",
    "\n",
    "# === MAIN ===\n",
    "if __name__ == '__main__':\n",
    "    df, backup = carregar_planilha(EXCEL_IN)\n",
    "    df, scores, faixas = calcular_coerencia(df)\n",
    "    plot_distribuicoes(scores)\n",
    "    wb, ws = salvar_com_backup(df, backup)\n",
    "    destacar_menores(df, ws)\n",
    "    formatar(ws)\n",
    "    wb.save(EXCEL_OUT)\n",
    "    wb.close()\n",
    "    relatorio_final(df, faixas, data_inicio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2914053b-d01a-44e9-9693-69b615dbc289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️ Início em: 2025-05-04 10:13:47 (Florianópolis | SC)\n",
      "\n",
      "📊 Rodando regressão OLS (H1a/b)...\n",
      "\n",
      "📋 Tabela de Regressão OLS (H1a/b):\n",
      "                     Variável  Coeficiente  Erro Padrão       t  p-valor Sig  IC 95% inferior  IC 95% superior\n",
      "                   Intercepto      -1.3833       0.5001 -2.7659   0.0057  **          -2.3637          -0.4029\n",
      "      Avaliação de desempenho      -0.0035       0.0061 -0.5686   0.5697              -0.0154           0.0085\n",
      " Comportamento Organizacional      -0.0218       0.0051 -4.2418   0.0000 ***          -0.0318          -0.0117\n",
      "                        Custo      -0.0233       0.0047 -4.9910   0.0000 ***          -0.0324          -0.0141\n",
      "            Educação Contábil      -0.0021       0.0074 -0.2762   0.7824              -0.0166           0.0125\n",
      "             Não Classificado      -0.0095       0.0043 -2.2137   0.0269   *          -0.0179          -0.0011\n",
      "                 Outros Temas       0.0027       0.0054  0.4996   0.6174              -0.0079           0.0132\n",
      "Sistema de Controle Gerencial      -0.0206       0.0053 -3.8808   0.0001 ***          -0.0309          -0.0102\n",
      "                          Ano       0.0010       0.0002  4.1473   0.0000 ***           0.0005           0.0015\n",
      "                      Quartil      -0.0099       0.0015 -6.3900   0.0000 ***          -0.0129          -0.0068\n",
      "\n",
      "📊 Resumo técnico do modelo OLS (sem coef. de país):\n",
      "\n",
      "                              OLS Regression Results                              \n",
      "==================================================================================\n",
      "Dep. Variable:     Q(\"Coerência (Score)\")   R-squared:                       0.071\n",
      "Model:                                OLS   Adj. R-squared:                  0.054\n",
      "Method:                     Least Squares   F-statistic:                     4.176\n",
      "Date:                    Sun, 04 May 2025   Prob (F-statistic):           9.82e-46\n",
      "Time:                            13:13:50   Log-Likelihood:                 5080.6\n",
      "No. Observations:                    6700   AIC:                            -9919.\n",
      "Df Residuals:                        6579   BIC:                            -9095.\n",
      "Df Model:                             120                                         \n",
      "Covariance Type:                nonrobust                                         \n",
      "===================================================================================================================================================\n",
      "                                                                                      coef    std err          t      P>|t|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Intercepto                                                                          -1.3833      0.500     -2.766      0.006      -2.364      -0.403\n",
      "C(Q(\"Grupo Temático\"), Treatment(\"Orçamento\"))[T.Avaliação de desempenho]          -0.0035      0.006     -0.569      0.570      -0.015       0.008\n",
      "C(Q(\"Grupo Temático\"), Treatment(\"Orçamento\"))[T.Comportamento Organizacional]     -0.0218      0.005     -4.242      0.000      -0.032      -0.012\n",
      "C(Q(\"Grupo Temático\"), Treatment(\"Orçamento\"))[T.Custo]                            -0.0233      0.005     -4.991      0.000      -0.032      -0.014\n",
      "C(Q(\"Grupo Temático\"), Treatment(\"Orçamento\"))[T.Educação Contábil]                -0.0021      0.007     -0.276      0.782      -0.017       0.013\n",
      "C(Q(\"Grupo Temático\"), Treatment(\"Orçamento\"))[T.Não Classificado]                 -0.0095      0.004     -2.214      0.027      -0.018      -0.001\n",
      "C(Q(\"Grupo Temático\"), Treatment(\"Orçamento\"))[T.Outros Temas]                      0.0027      0.005      0.500      0.617      -0.008       0.013\n",
      "C(Q(\"Grupo Temático\"), Treatment(\"Orçamento\"))[T.Sistema de Controle Gerencial]    -0.0206      0.005     -3.881      0.000      -0.031      -0.010\n",
      "Ano                                                                           0.0010      0.000      4.147      0.000       0.001       0.001\n",
      "Quartil                                                                 -0.0099      0.002     -6.390      0.000      -0.013      -0.007\n",
      "==============================================================================\n",
      "Omnibus:                      565.483   Durbin-Watson:                   1.958\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              756.090\n",
      "Skew:                          -0.717   Prob(JB):                    6.56e-165\n",
      "Kurtosis:                       3.807   Cond. No.                     1.75e+06\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.75e+06. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "Significance levels: * p<0.05, ** p<0.01, *** p<0.001\n",
      "\n",
      "📈 Gerando heatmap com Top 20 países...\n",
      "📊 Gerando estatísticas por país...\n",
      "\n",
      "📁 Arquivos gerados:\n",
      "  • H1ab_Regressao_OLS_Resultados.xlsx\n",
      "  • H1ab_Heatmap_Top20_Paises.png\n",
      "  • H1ab_Resumo_Por_Pais.xlsx\n",
      "\n",
      "===== RELATÓRIO FINAL =====\n",
      "Tempo de execução: 0:00:05\n",
      "Finalizado em: 2025-05-04 10:13:53 (Florianópolis | SC)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "h1ab_regressao_OLS_orcamento.py\n",
    "\n",
    "Script 6 para:\n",
    "  - Executar análise OLS das hipóteses H1a/H1b com controle por país, ano e quartil.\n",
    "  - Exporta:\n",
    "  - Tabela de regressão (limpa) com significância no estilo R (*, **, ***).\n",
    "  - Heatmap dos Top 20 países (Coerência por País e Ano).\n",
    "  - Estatísticas descritivas por país.\n",
    "\n",
    "Inclui print técnico do modelo sem coeficientes de país e com nomes legíveis.\n",
    "\n",
    "USO NO CLUSTER UFSC (JupyterLab ou SLURM):\n",
    "  • Instale dependências: pip install pandas statsmodels openpyxl seaborn matplotlib\n",
    "  • No cluster: module load python/3.10 pandas statsmodels openpyxl seaborn matplotlib\n",
    "  • Execute: python h1ab_regressao_OLS_orcamento.py\n",
    "  • Arquivo \"Portfolio_analitico_coerencia.xlsx\" deve estar no diretório atual.\n",
    "\n",
    "DEPENDÊNCIAS:\n",
    "  pandas, statsmodels, openpyxl, seaborn, matplotlib\n",
    "\n",
    "Autor: G.O., Renato | NEIMAC | PPGC | UFSC | Mestrado\n",
    "Contribuidores: [Adicione seu nome, se aplicável]\n",
    "Licença: MIT (Ciência Aberta - Open Science Framework)\n",
    "Data: 02/05/2025\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "import re\n",
    "\n",
    "# Função para instalar pacotes, se necessário\n",
    "def instalar(pkg):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "\n",
    "for pkg in (\"pandas\", \"statsmodels\", \"openpyxl\", \"seaborn\", \"matplotlib\"):\n",
    "    try:\n",
    "        __import__(pkg.replace('-', '_'))\n",
    "    except ImportError:\n",
    "        print(f\"⚙️ Instalando {pkg}...\")\n",
    "        instalar(pkg)\n",
    "\n",
    "# Função para marcar significância no estilo R\n",
    "def marcar_sig(p):\n",
    "    if p < 0.001:\n",
    "        return '***'\n",
    "    elif p < 0.01:\n",
    "        return '**'\n",
    "    elif p < 0.05:\n",
    "        return '*'\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# Função para limpar nomes das variáveis\n",
    "def limpar_nome_variavel(v):\n",
    "    if 'Grupo Temático' in v:\n",
    "        if 'T.' in v:\n",
    "            return v.split('T.')[1].strip(']')\n",
    "        else:\n",
    "            return 'Orçamento'  # ALTERAÇÃO: Mudei de 'Avaliação de Desempenho' pra 'Orçamento'\n",
    "    elif v == 'Intercept':\n",
    "        return 'Intercepto'\n",
    "    elif v == 'Q(\"Year\")':\n",
    "        return 'Ano'\n",
    "    elif v == 'Q(\"Best Quartile\")':\n",
    "        return 'Quartil'\n",
    "    return v\n",
    "\n",
    "# Início\n",
    "SC_TZ = ZoneInfo(\"America/Sao_Paulo\")\n",
    "inicio = datetime.now(SC_TZ)\n",
    "print(f\"▶️ Início em: {inicio:%Y-%m-%d %H:%M:%S} (Florianópolis | SC)\\n\")\n",
    "\n",
    "# Leitura do arquivo\n",
    "arquivo = Path(\"Portfolio_analitico_coerencia.xlsx\")\n",
    "if not arquivo.exists():\n",
    "    sys.exit(f\"⛔ Arquivo não encontrado: {arquivo}\")\n",
    "df = pd.read_excel(arquivo)\n",
    "\n",
    "# Preparação dos dados\n",
    "df = df[['Coerência (Score)', 'Grupo Temático', 'Year', 'País', 'Best Quartile']].dropna()\n",
    "df = df[df['Best Quartile'].str.startswith('Q')]\n",
    "df['Best Quartile'] = df['Best Quartile'].str.replace('Q', '').astype(int)\n",
    "df = df[df['País'].str.isalpha() & ~df['País'].isin(['Não Identificado'])]\n",
    "\n",
    "# Regressão OLS\n",
    "print(\"📊 Rodando regressão OLS (H1a/b)...\")\n",
    "# ALTERAÇÃO: Adicionei Treatment(\"Orçamento\") pra definir Orçamento como referência\n",
    "formula = 'Q(\"Coerência (Score)\") ~ C(Q(\"Grupo Temático\"), Treatment(\"Orçamento\")) + Q(\"Year\") + C(Q(\"País\")) + Q(\"Best Quartile\")'\n",
    "modelo = smf.ols(formula=formula, data=df).fit()\n",
    "\n",
    "# Tabela de saída formatada no estilo R\n",
    "tabela = modelo.summary2().tables[1].reset_index()\n",
    "# Removemos o filtro para incluir todas as variáveis (não apenas Grupo Temático)\n",
    "tabela = tabela[~tabela['index'].str.contains('País')].copy()  # Exclui apenas os coeficientes de País\n",
    "tabela.columns = ['Variável', 'Coeficiente', 'Erro Padrão', 't', 'p-valor', 'IC 95% inferior', 'IC 95% superior']\n",
    "tabela['Variável'] = tabela['Variável'].apply(limpar_nome_variavel)\n",
    "tabela['Sig'] = tabela['p-valor'].apply(marcar_sig)\n",
    "tabela = tabela[['Variável', 'Coeficiente', 'Erro Padrão', 't', 'p-valor', 'Sig', 'IC 95% inferior', 'IC 95% superior']]\n",
    "tabela = tabela.round(4)\n",
    "tabela.to_excel(\"H1ab_Regressao_OLS_Resultados.xlsx\", index=False)\n",
    "\n",
    "# Imprimir a tabela no console\n",
    "print(\"\\n📋 Tabela de Regressão OLS (H1a/b):\")\n",
    "print(tabela.to_string(index=False))\n",
    "\n",
    "# Resumo técnico limpo (sem coeficientes de país e com nomes legíveis)\n",
    "print(\"\\n📊 Resumo técnico do modelo OLS (sem coef. de país):\\n\")\n",
    "resumo_str = modelo.summary().as_text()\n",
    "resumo_limpo = re.sub(\n",
    "    r'C\\(Q\\(\"País\"\\)\\)\\[T\\..+?\\]\\s+[-\\d\\.e\\+]+\\s+[-\\d\\.e\\+]+\\s+[-\\d\\.e\\+]+\\s+[-\\d\\.e\\+]+\\s+[-\\d\\.e\\+]+\\s+[-\\d\\.e\\+]+\\n',\n",
    "    '', resumo_str)\n",
    "\n",
    "# Substituir nomes técnicos por nomes legíveis\n",
    "substituir_variaveis = {\n",
    "    'C\\\\(Q\\\\(\"Grupo Temático\"\\\\)\\\\)\\\\[T.Comportamento Organizacional\\\\]': \"Comportamento Organizacional\",\n",
    "    'C\\\\(Q\\\\(\"Grupo Temático\"\\\\)\\\\)\\\\[T.Custo\\\\]': \"Custo\",\n",
    "    'C\\\\(Q\\\\(\"Grupo Temático\"\\\\)\\\\)\\\\[T.Educação Contábil\\\\]': \"Educação Contábil\",\n",
    "    'C\\\\(Q\\\\(\"Grupo Temático\"\\\\)\\\\)\\\\[T.Não Classificado\\\\]': \"Não Classificado\",\n",
    "    'C\\\\(Q\\\\(\"Grupo Temático\"\\\\)\\\\)\\\\[T.Orçamento\\\\]': \"Orçamento\",\n",
    "    'C\\\\(Q\\\\(\"Grupo Temático\"\\\\)\\\\)\\\\[T.Outros Temas\\\\]': \"Outros Temas\",\n",
    "    'C\\\\(Q\\\\(\"Grupo Temático\"\\\\)\\\\)\\\\[T.Sistema de Controle Gerencial\\\\]': \"Sistema de Controle Gerencial\",\n",
    "    'Q\\\\(\"Year\"\\\\)': \"Ano\",\n",
    "    'Q\\\\(\"Best Quartile\"\\\\)': \"Quartil\",\n",
    "    'Intercept': 'Intercepto'\n",
    "}\n",
    "for padrao, nome_limpo in substituir_variaveis.items():\n",
    "    resumo_limpo = re.sub(padrao, nome_limpo, resumo_limpo)\n",
    "\n",
    "# Adicionar nota sobre significância\n",
    "resumo_limpo += \"\\nSignificance levels: * p<0.05, ** p<0.01, *** p<0.001\"\n",
    "print(resumo_limpo)\n",
    "\n",
    "# Heatmap dos Top 20 países\n",
    "print(\"\\n📈 Gerando heatmap com Top 20 países...\")\n",
    "df_heat = df[['País', 'Year', 'Coerência (Score)']].dropna()\n",
    "df_heat = df_heat[df_heat['Year'].between(1995, 2025)]\n",
    "top20 = df_heat['País'].value_counts().nlargest(20).index.tolist()\n",
    "df_heat = df_heat[df_heat['País'].isin(top20)]\n",
    "pivot = df_heat.groupby(['País', 'Year'])['Coerência (Score)'].mean().reset_index().pivot(\n",
    "    index='País', columns='Year', values='Coerência (Score)'\n",
    ")\n",
    "pivot = pivot.reindex(columns=range(1995, 2026))\n",
    "plt.figure(figsize=(18, 10))\n",
    "sns.heatmap(pivot, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", linewidths=0.5, linecolor='gray')\n",
    "plt.title(\"Média do Score de Coerência por País × Ano (Top 20 Países)\")\n",
    "plt.xlabel(\"Ano\")\n",
    "plt.ylabel(\"País\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"H1ab_Heatmap_Top20_Paises.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Estatísticas descritivas por país\n",
    "print(\"📊 Gerando estatísticas por país...\")\n",
    "agg = df.groupby('País')['Coerência (Score)'].agg(['count', 'mean', 'std', 'median', 'min', 'max']).round(3)\n",
    "agg.columns = ['N Artigos', 'Média', 'Desvio Padrão', 'Mediana', 'Mínimo', 'Máximo']\n",
    "agg = agg.sort_values('Média', ascending=False)\n",
    "agg.to_excel(\"H1ab_Resumo_Por_Pais.xlsx\")\n",
    "\n",
    "# Relatório final\n",
    "fim = datetime.now(SC_TZ)\n",
    "dur = str(fim - inicio).split('.')[0]\n",
    "print(\"\\n📁 Arquivos gerados:\")\n",
    "print(\"  • H1ab_Regressao_OLS_Resultados.xlsx\")\n",
    "print(\"  • H1ab_Heatmap_Top20_Paises.png\")\n",
    "print(\"  • H1ab_Resumo_Por_Pais.xlsx\")\n",
    "print(\"\\n===== RELATÓRIO FINAL =====\")\n",
    "print(f\"Tempo de execução: {dur}\")\n",
    "print(f\"Finalizado em: {fim:%Y-%m-%d %H:%M:%S} (Florianópolis | SC)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0221539f-c0c0-4076-94ed-d098050090e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️ Início em: 2025-05-04 14:06:06 (Florianópolis | SC)\n",
      "\n",
      "📊 Rodando regressão OLS (H2a/b)...\n",
      "🔍 Variáveis antes da ordenação: ['Intercepto', '(Q1-Q2)', 'Ano']\n",
      "\n",
      "📋 Tabela de Regressão OLS (H2a/b):\n",
      "Dependente: Coerência (Score)\n",
      "Nota: (Q3-Q4) é a referência (coeficiente 0).\n",
      "  Variável  Coeficiente  Erro Padrão       t  p-valor Sig  IC 95% inferior  IC 95% superior\n",
      "Intercepto      -1.2666       0.4988 -2.5394   0.0111   *          -2.2444          -0.2889\n",
      "       Ano       0.0009       0.0002  3.7948   0.0001 ***           0.0004           0.0014\n",
      "   (Q1-Q2)       0.0207       0.0035  5.9960   0.0000 ***           0.0140           0.0275\n",
      "   (Q3-Q4)       0.0000       0.0000  0.0000   1.0000               0.0000           0.0000\n",
      "\n",
      "📊 Resumo técnico do modelo OLS (sem coef. de país):\n",
      "\n",
      "                              OLS Regression Results                              \n",
      "==================================================================================\n",
      "Dep. Variable: Coerência (Score)\n",
      "Model:                                OLS   Adj. R-squared:                  0.047\n",
      "Method:                     Least Squares   F-statistic:                     3.932\n",
      "Date:                    Sun, 04 May 2025   Prob (F-statistic):           4.44e-39\n",
      "Time:                            17:06:08   Log-Likelihood:                 5053.4\n",
      "No. Observations:                    6700   AIC:                            -9879.\n",
      "Df Residuals:                        6586   BIC:                            -9102.\n",
      "Df Model:                             113                                         \n",
      "Covariance Type:                nonrobust                                         \n",
      "=================================================================================================\n",
      "                                    coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------\n",
      "Intercepto                        -1.2666      0.499     -2.539      0.011      -2.244      -0.289\n",
      "(Q1-Q2)         0.0207      0.003      5.996      0.000       0.014       0.028\n",
      "Ano                         0.0009      0.000      3.795      0.000       0.000       0.001\n",
      "==============================================================================\n",
      "Omnibus:                      579.289   Durbin-Watson:                   1.954\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              777.266\n",
      "Skew:                          -0.729   Prob(JB):                    1.66e-169\n",
      "Kurtosis:                       3.812   Cond. No.                     1.75e+06\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.75e+06. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "Significance levels: * p<0.05, ** p<0.01, *** p<0.001\n",
      "\n",
      "📈 Gerando heatmap com Top 20 países...\n",
      "📊 Gerando estatísticas por país...\n",
      "\n",
      "📁 Arquivos gerados:\n",
      "  • H2ab_Regressao_OLS_Resultados.xlsx\n",
      "  • H2ab_Heatmap_Top20_Paises.png\n",
      "  • H2ab_Resumo_Por_Pais.xlsx\n",
      "\n",
      "===== RELATÓRIO FINAL =====\n",
      "Tempo de execução: 0:00:03\n",
      "Finalizado em: 2025-05-04 14:06:10 (Florianópolis | SC)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "h2ab_regressao_OLS_analise_completa.py\n",
    "\n",
    "Script 7 para:\n",
    "  - Executar análise OLS das hipóteses H2a/H2b, investigando a relação entre o impacto\n",
    "    dos periódicos ((Q1-Q2) vs. (Q3-Q4)) e a coerência comunicacional, controlando por\n",
    "    país e ano. Exporta:\n",
    "  - Tabela de regressão com significância no estilo R (*, **, ***).\n",
    "  - Heatmap dos Top 20 países (Coerência por Ano).\n",
    "  - Estatísticas descritivas por país.\n",
    "\n",
    "USO NO CLUSTER UFSC (JupyterLab ou SLURM):\n",
    "  • Instale dependências: pip install pandas statsmodels openpyxl seaborn matplotlib\n",
    "  • No cluster: module load python/3.10 pandas statsmodels openpyxl seaborn matplotlib\n",
    "  • Execute: python h2ab_regressao_OLS_analise_completa.py\n",
    "  • Arquivo \"Portfolio_analitico_coerencia.xlsx\" deve estar no diretório atual.\n",
    "\n",
    "DEPENDÊNCIAS:\n",
    "  pandas, statsmodels, openpyxl, seaborn, matplotlib\n",
    "\n",
    "Autor: G.O., Renato | NEIMAC | PPGC | UFSC | Mestrado\n",
    "Licença: MIT (Ciência Aberta - Open Science Framework)\n",
    "Data: 02/05/2025\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "import re\n",
    "\n",
    "# Função para instalar pacotes, se necessário\n",
    "def instalar(pkg):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "\n",
    "for pkg in (\"pandas\", \"statsmodels\", \"openpyxl\", \"seaborn\", \"matplotlib\"):\n",
    "    try:\n",
    "        __import__(pkg.replace('-', '_'))\n",
    "    except ImportError:\n",
    "        print(f\"⚙️ Instalando {pkg}...\")\n",
    "        instalar(pkg)\n",
    "\n",
    "# Função para marcar significância no estilo R\n",
    "def marcar_sig(p):\n",
    "    if p < 0.001:\n",
    "        return '***'\n",
    "    elif p < 0.01:\n",
    "        return '**'\n",
    "    elif p < 0.05:\n",
    "        return '*'\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# Função para limpar nomes das variáveis\n",
    "def limpar_nome_variavel(v):\n",
    "    if v == 'Intercept':\n",
    "        return 'Intercepto'\n",
    "    elif v == 'Q(\"Year\")':\n",
    "        return 'Ano'\n",
    "    elif v == 'C(Q(\"Quartil_Alto\"))[T.1]':\n",
    "        return '(Q1-Q2)'\n",
    "    return v\n",
    "\n",
    "# Função para extrair o número do quartil para ordenação\n",
    "def extrair_quartil(v):\n",
    "    if v == 'Intercepto':\n",
    "        return -2  # Intercepto primeiro\n",
    "    elif v == 'Ano':\n",
    "        return -1  # Ano depois do Intercepto\n",
    "    elif v == '(Q1-Q2)':\n",
    "        return 1  # (Q1-Q2) primeiro\n",
    "    elif v == '(Q3-Q4)':\n",
    "        return 2  # (Q3-Q4) depois\n",
    "    return 0  # Outros casos (não deve ocorrer)\n",
    "\n",
    "# Início\n",
    "SC_TZ = ZoneInfo(\"America/Sao_Paulo\")\n",
    "inicio = datetime.now(SC_TZ)\n",
    "print(f\"▶️ Início em: {inicio:%Y-%m-%d %H:%M:%S} (Florianópolis | SC)\\n\")\n",
    "\n",
    "# Leitura do arquivo\n",
    "arquivo = Path(\"Portfolio_analitico_coerencia.xlsx\")\n",
    "if not arquivo.exists():\n",
    "    sys.exit(f\"⛔ Arquivo não encontrado: {arquivo}\")\n",
    "df = pd.read_excel(arquivo)\n",
    "\n",
    "# Preparação dos dados\n",
    "df = df[['Coerência (Score)', 'Best Quartile', 'Year', 'País']].dropna()\n",
    "df = df[df['Best Quartile'].str.startswith('Q')]\n",
    "df['Best Quartile'] = df['Best Quartile'].str.replace('Q', '').astype(int)\n",
    "df = df[df['País'].str.isalpha() & ~df['País'].isin(['Não Identificado'])]\n",
    "\n",
    "# Criar dummy para Quartil Alto (Q1-Q2 = 1, Q3-Q4 = 0)\n",
    "df['Quartil_Alto'] = (df['Best Quartile'] <= 2).astype(int)\n",
    "\n",
    "# Regressão OLS\n",
    "print(\"📊 Rodando regressão OLS (H2a/b)...\")\n",
    "formula = 'Q(\"Coerência (Score)\") ~ C(Q(\"Quartil_Alto\")) + Q(\"Year\") + C(Q(\"País\"))'\n",
    "modelo = smf.ols(formula=formula, data=df).fit()\n",
    "\n",
    "# Tabela de saída formatada no estilo R\n",
    "tabela = modelo.summary2().tables[1].reset_index()\n",
    "tabela = tabela[~tabela['index'].str.contains('País')].copy()  # Exclui apenas os coeficientes de País\n",
    "tabela.columns = ['Variável', 'Coeficiente', 'Erro Padrão', 't', 'p-valor', 'IC 95% inferior', 'IC 95% superior']\n",
    "tabela['Variável'] = tabela['Variável'].apply(limpar_nome_variavel)\n",
    "print(\"🔍 Variáveis antes da ordenação:\", tabela['Variável'].tolist())  # Log para depuração\n",
    "tabela['Sig'] = tabela['p-valor'].apply(marcar_sig)\n",
    "# Adicionar linha de referência (Q3-Q4) manualmente\n",
    "ref_row = pd.DataFrame({\n",
    "    'Variável': ['(Q3-Q4)'],\n",
    "    'Coeficiente': [0.0000],\n",
    "    'Erro Padrão': [0.0000],\n",
    "    't': [0.0000],\n",
    "    'p-valor': [1.0000],\n",
    "    'IC 95% inferior': [0.0000],\n",
    "    'IC 95% superior': [0.0000],\n",
    "    'Sig': ['']\n",
    "})\n",
    "tabela = pd.concat([tabela, ref_row], ignore_index=True)\n",
    "# Reordenar para listar Intercepto, Ano, (Q1-Q2), (Q3-Q4)\n",
    "tabela['Ordem'] = tabela['Variável'].apply(extrair_quartil)\n",
    "tabela = tabela.sort_values('Ordem')\n",
    "tabela = tabela[['Variável', 'Coeficiente', 'Erro Padrão', 't', 'p-valor', 'Sig', 'IC 95% inferior', 'IC 95% superior']]\n",
    "tabela = tabela.round(4)\n",
    "tabela.to_excel(\"H2ab_Regressao_OLS_Resultados.xlsx\", index=False)\n",
    "\n",
    "# Imprimir a tabela no console\n",
    "print(\"\\n📋 Tabela de Regressão OLS (H2a/b):\")\n",
    "print(\"Dependente: Coerência (Score)\")\n",
    "print(\"Nota: (Q3-Q4) é a referência (coeficiente 0).\")\n",
    "print(tabela.to_string(index=False))\n",
    "\n",
    "# Resumo técnico limpo (sem coeficientes de país e com nomes legíveis)\n",
    "print(\"\\n📊 Resumo técnico do modelo OLS (sem coef. de país):\\n\")\n",
    "resumo_str = modelo.summary().as_text()\n",
    "resumo_limpo = re.sub(\n",
    "    r'C\\(Q\\(\"País\"\\)\\)\\[T\\..+?\\]\\s+[-\\d\\.e\\+]+\\s+[-\\d\\.e\\+]+\\s+[-\\d\\.e\\+]+\\s+[-\\d\\.e\\+]+\\s+[-\\d\\.e\\+]+\\s+[-\\d\\.e\\+]+\\n',\n",
    "    '', resumo_str)\n",
    "substituir_variaveis = {\n",
    "    r'C\\(Q\\(\"Quartil_Alto\"\\)\\)\\[T\\.1\\]': \"(Q1-Q2)\",\n",
    "    r'Q\\(\"Year\"\\)': \"Ano\",\n",
    "    r'Intercept': 'Intercepto'\n",
    "}\n",
    "for padrao, nome_limpo in substituir_variaveis.items():\n",
    "    resumo_limpo = re.sub(padrao, nome_limpo, resumo_limpo)\n",
    "resumo_limpo = re.sub(r'Dep\\. Variable:.*\\n', 'Dep. Variable: Coerência (Score)\\n', resumo_limpo)\n",
    "resumo_limpo += \"\\nSignificance levels: * p<0.05, ** p<0.01, *** p<0.001\"\n",
    "print(resumo_limpo)\n",
    "\n",
    "# Heatmap dos Top 20 países\n",
    "print(\"\\n📈 Gerando heatmap com Top 20 países...\")\n",
    "df_heat = df[['País', 'Year', 'Coerência (Score)']].dropna()\n",
    "df_heat = df_heat[df_heat['Year'].between(1995, 2025)]\n",
    "top20 = df_heat['País'].value_counts().nlargest(20).index.tolist()\n",
    "df_heat = df_heat[df_heat['País'].isin(top20)]\n",
    "pivot = df_heat.groupby(['País', 'Year'])['Coerência (Score)'].mean().reset_index().pivot(\n",
    "    index='País', columns='Year', values='Coerência (Score)'\n",
    ")\n",
    "pivot = pivot.reindex(columns=range(1995, 2026))\n",
    "plt.figure(figsize=(18, 10))\n",
    "sns.heatmap(pivot, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", linewidths=0.5, linecolor='gray')\n",
    "plt.title(\"Média do Score de Coerência por País × Ano (Top 20 Países)\")\n",
    "plt.xlabel(\"Ano\")\n",
    "plt.ylabel(\"País\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"H2ab_Heatmap_Top20_Paises.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Estatísticas descritivas por país\n",
    "print(\"📊 Gerando estatísticas por país...\")\n",
    "agg = df.groupby('País')['Coerência (Score)'].agg(['count', 'mean', 'std', 'median', 'min', 'max']).round(3)\n",
    "agg.columns = ['N Artigos', 'Média', 'Desvio Padrão', 'Mediana', 'Mínimo', 'Máximo']\n",
    "agg = agg.sort_values('Média', ascending=False)\n",
    "agg.to_excel(\"H2ab_Resumo_Por_Pais.xlsx\")\n",
    "\n",
    "# Relatório final\n",
    "fim = datetime.now(SC_TZ)\n",
    "dur = str(fim - inicio).split('.')[0]\n",
    "print(\"\\n📁 Arquivos gerados:\")\n",
    "print(\"  • H2ab_Regressao_OLS_Resultados.xlsx\")\n",
    "print(\"  • H2ab_Heatmap_Top20_Paises.png\")\n",
    "print(\"  • H2ab_Resumo_Por_Pais.xlsx\")\n",
    "print(\"\\n===== RELATÓRIO FINAL =====\")\n",
    "print(f\"Tempo de execução: {dur}\")\n",
    "print(f\"Finalizado em: {fim:%Y-%m-%d %H:%M:%S} (Florianópolis | SC)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ac3bbd3-ecd2-4f38-872a-8dd5bca03d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶️ Início em: 2025-05-04 15:55:02 (Florianópolis | SC)\n",
      "\n",
      "📊 Preparando dados...\n",
      "🔍 Tratando valores extremos em Cited by...\n",
      "🔍 Verificando dados...\n",
      "  • Observações: 13614\n",
      "  • Zeros em Cited by: 15.5%\n",
      "  • Categorias em Classificação de Coerência: {'Média': 8573, 'Baixa': 3862, 'Alta': 1179}\n",
      "  • Categorias em Best Quartile: {'Q1': 4721, 'NE': 3101, 'Q2': 3090, 'Q3': 1621, 'Q4': 1081}\n",
      "📊 Rodando regressão OLS (H3a/b)...\n",
      "\n",
      "📋 Tabela de regressão OLS (H3a/b):\n",
      "🔍 Colunas em tabela_ols: ['index', 'Coef.', 'Std.Err.', 'z', 'P>|z|', '[0.025', '0.975]', 'Variável']\n",
      "Dependente: log(1 + Cited by) (OLS)\n",
      "                             Variável  Coeficiente  Erro Padrão      z/t  p-valor Sig  IC 95% inferior  IC 95% superior\n",
      "              Alta (coerência ≥ 0,80)       0.0000       0.0000   0.0000    1.000               0.0000           0.0000\n",
      "             Baixa (coerência < 0,60)      -0.2019       0.0382  -5.2847    0.000 ***          -0.2768          -0.1270\n",
      "Média (coerência entre 0,60 e < 0,80)      -0.0599       0.0359  -1.6698    0.095              -0.1302           0.0104\n",
      "                           Intercepto     157.1231       3.3138  47.4152    0.000 ***         150.6282         163.6180\n",
      "                                  Ano      -0.0770       0.0016 -46.9122    0.000 ***          -0.0802          -0.0738\n",
      "                           Quartil Q1       1.0779       0.0285  37.7838    0.000 ***           1.0220           1.1339\n",
      "                           Quartil Q2       0.2433       0.0301   8.0831    0.000 ***           0.1843           0.3023\n",
      "                           Quartil Q3      -0.1409       0.0344  -4.0997    0.000 ***          -0.2083          -0.0736\n",
      "                           Quartil Q4      -0.6576       0.0347 -18.9552    0.000 ***          -0.7256          -0.5896\n",
      "\n",
      "📊 Resumo técnico do modelo:\n",
      "\n",
      "Modelo OLS:\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable: log(1 + Cited by)\n",
      "Model:                            OLS   Adj. R-squared:                  0.362\n",
      "Method:                 Least Squares   F-statistic:                     1212.\n",
      "Date:                Sun, 04 May 2025   Prob (F-statistic):               0.00\n",
      "Time:                        18:55:04   Log-Likelihood:                -20973.\n",
      "No. Observations:               13614   AIC:                         4.196e+04\n",
      "Df Residuals:                   13606   BIC:                         4.202e+04\n",
      "Df Model:                           7                                         \n",
      "Covariance Type:                  HC3                                         \n",
      "===============================================================================================================\n",
      "                                                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "Intercepto                                     157.1231      3.314     47.415      0.000     150.628     163.618\n",
      "Baixa    -0.2019      0.038     -5.285      0.000      -0.277      -0.127\n",
      "Média    -0.0599      0.036     -1.670      0.095      -0.130       0.010\n",
      "Quartil Q1                     1.0779      0.029     37.784      0.000       1.022       1.134\n",
      "Quartil Q2                     0.2433      0.030      8.083      0.000       0.184       0.302\n",
      "Quartil Q3                    -0.1409      0.034     -4.100      0.000      -0.208      -0.074\n",
      "Quartil Q4                    -0.6576      0.035    -18.955      0.000      -0.726      -0.590\n",
      "Ano                                      -0.0770      0.002    -46.912      0.000      -0.080      -0.074\n",
      "==============================================================================\n",
      "Omnibus:                       96.084   Durbin-Watson:                   1.592\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               72.628\n",
      "Skew:                          -0.085   Prob(JB):                     1.69e-16\n",
      "Kurtosis:                       2.685   Cond. No.                     6.00e+05\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC3)\n",
      "[2] The condition number is large,  6e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "Significance levels: * p<0.05, ** p<0.01, *** p<0.001\n",
      "\n",
      "📊 Gerando estatísticas por Classificação de Coerência...\n",
      "\n",
      "📁 Arquivos gerados:\n",
      "  • H3ab_Regressao_Comparativa_Resultados.xlsx\n",
      "  • H3ab_Resumo_Por_Coerencia.xlsx\n",
      "\n",
      "===== RELATÓRIO FINAL =====\n",
      "Tempo de execução: 0:00:01\n",
      "Finalizado em: 2025-05-04 15:55:04 (Florianópolis | SC)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "h3ab_regressao_OLS_analise_completa_corrigido.py\n",
    "\n",
    "Script 8 para:\n",
    "  - Executar análise de Regressão OLS para as hipóteses H3a/H3b, investigando a relação\n",
    "    entre a coerência comunicacional (Classificação de Coerência: Alta vs. Média/Baixa)\n",
    "    e o número de citações (log(1 + Cited by)), controlando por Best Quartile e Ano.\n",
    "    Exporta:\n",
    "  - Tabela de regressão OLS com significância no estilo R (*, **, ***), com variáveis de controle separadas e descrições detalhadas.\n",
    "  - Estatísticas descritivas por Classificação de Coerência.\n",
    "  - Resumo técnico do modelo.\n",
    "\n",
    "USO NO CLUSTER UFSC (JupyterLab ou SLURM):\n",
    "  • Instale dependências: pip install pandas statsmodels openpyxl numpy scipy\n",
    "  • No cluster: module load python/3.10 pandas statsmodels openpyxl numpy scipy\n",
    "  • Execute: python h3ab_regressao_OLS_analise_completa_corrigido.py\n",
    "  • Arquivo \"Portfolio_analitico_coerencia.xlsx\" deve estar no diretório atual.\n",
    "\n",
    "DEPENDÊNCIAS:\n",
    "  pandas, statsmodels, openpyxl, numpy, scipy\n",
    "\n",
    "Autor: G.O., Renato | NEIMAC | PPGC | UFSC | Mestrado\n",
    "Licença: MIT (Ciência Aberta - Open Science Framework)\n",
    "Data: 03/05/2025\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as stats\n",
    "import re\n",
    "\n",
    "# Função para instalar pacotes, se necessário\n",
    "def instalar(pkg):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "\n",
    "for pkg in (\"pandas\", \"statsmodels\", \"openpyxl\", \"numpy\", \"scipy\"):\n",
    "    try:\n",
    "        __import__(pkg.replace('-', '_'))\n",
    "    except ImportError:\n",
    "        print(f\"⚙️ Instalando {pkg}...\")\n",
    "        instalar(pkg)\n",
    "\n",
    "# Função para marcar significância no estilo R\n",
    "def marcar_sig(p):\n",
    "    if p < 0.001:\n",
    "        return '***'\n",
    "    elif p < 0.01:\n",
    "        return '**'\n",
    "    elif p < 0.05:\n",
    "        return '*'\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# Função para limpar nomes das variáveis e adicionar descrições\n",
    "def limpar_nome_variavel_com_descricao(v):\n",
    "    if v == 'Intercept':\n",
    "        return 'Intercepto'\n",
    "    elif v == 'Q(\"Year\")':\n",
    "        return 'Ano'\n",
    "    elif v.startswith('C(Q(\"Classificação de Coerência\"))[T.'):\n",
    "        categoria = v.split('T.')[1].split(']')[0]\n",
    "        if categoria == 'Alta':\n",
    "            return 'Alta (coerência ≥ 0,80)'\n",
    "        elif categoria == 'Baixa':\n",
    "            return 'Baixa (coerência < 0,60)'\n",
    "        elif categoria == 'Média':\n",
    "            return 'Média (coerência entre 0,60 e < 0,80)'\n",
    "    elif v.startswith('C(Q(\"Best Quartile\"))[T.'):\n",
    "        return f\"Quartil {v.split('T.')[1].split(']')[0]}\"\n",
    "    return v\n",
    "\n",
    "# Função para extrair ordem das variáveis e identificar se é de controle\n",
    "def extrair_ordem_variavel(v):\n",
    "    if v == 'Intercepto':\n",
    "        return -2, True\n",
    "    elif v == 'Ano':\n",
    "        return -1, True\n",
    "    elif v.startswith('Alta'):\n",
    "        return 1, False\n",
    "    elif v.startswith('Baixa'):\n",
    "        return 2, False\n",
    "    elif v.startswith('Média'):\n",
    "        return 3, False\n",
    "    elif v.startswith('Quartil'):\n",
    "        if v == 'Quartil NE':\n",
    "            return 4, True\n",
    "        try:\n",
    "            return 4 + int(v.split(' ')[1][1:]), True  # Extrai número de \"Q1\", \"Q2\", etc.\n",
    "        except (ValueError, IndexError):\n",
    "            return 4, True  # Trata valores não numéricos como NE\n",
    "    return 100, False\n",
    "\n",
    "# Início\n",
    "SC_TZ = ZoneInfo(\"America/Sao_Paulo\")\n",
    "inicio = datetime.now(SC_TZ)\n",
    "print(f\"▶️ Início em: {inicio:%Y-%m-%d %H:%M:%S} (Florianópolis | SC)\\n\")\n",
    "\n",
    "# Leitura do arquivo\n",
    "arquivo = Path(\"Portfolio_analitico_coerencia.xlsx\")\n",
    "if not arquivo.exists():\n",
    "    sys.exit(f\"⛔ Arquivo não encontrado: {arquivo}\")\n",
    "df = pd.read_excel(arquivo)\n",
    "\n",
    "# Preparação dos dados\n",
    "print(\"📊 Preparando dados...\")\n",
    "df = df[['Cited by', 'Classificação de Coerência', 'Best Quartile', 'Year']]\n",
    "df['Best Quartile'] = df['Best Quartile'].fillna('NE').replace(['-', 'Não Encontrado'], 'NE')\n",
    "df['Classificação de Coerência'] = df['Classificação de Coerência'].fillna('Média')\n",
    "df['Year'] = df['Year'].fillna(df['Year'].median())\n",
    "df['Cited by'] = df['Cited by'].fillna(0).astype(int)\n",
    "\n",
    "# Tratamento de valores extremos em Cited by (winsorização no top 5%)\n",
    "print(\"🔍 Tratando valores extremos em Cited by...\")\n",
    "cited_by_winsorized = stats.mstats.winsorize(df['Cited by'], limits=[0, 0.05])\n",
    "df['Cited by'] = cited_by_winsorized\n",
    "df['Log_Cited_by'] = np.log1p(df['Cited by'])\n",
    "\n",
    "# Verificação de dados\n",
    "print(\"🔍 Verificando dados...\")\n",
    "print(f\"  • Observações: {len(df)}\")\n",
    "print(f\"  • Zeros em Cited by: {100 * (df['Cited by'] == 0).mean():.1f}%\")\n",
    "print(f\"  • Categorias em Classificação de Coerência: {df['Classificação de Coerência'].value_counts().to_dict()}\")\n",
    "print(f\"  • Categorias em Best Quartile: {df['Best Quartile'].value_counts().to_dict()}\")\n",
    "for col in ['Best Quartile', 'Classificação de Coerência']:\n",
    "    rare_cats = df[col].value_counts()[df[col].value_counts() < 100].index\n",
    "    if len(rare_cats) > 0:\n",
    "        print(f\"  • Aviso: Categorias raras em {col} (<100 observações): {list(rare_cats)}\")\n",
    "\n",
    "# Regressão OLS (modelo principal)\n",
    "print(\"📊 Rodando regressão OLS (H3a/b)...\")\n",
    "formula_ols = 'Q(\"Log_Cited_by\") ~ C(Q(\"Classificação de Coerência\")) + C(Q(\"Best Quartile\")) + Q(\"Year\")'\n",
    "modelo_ols = smf.ols(formula=formula_ols, data=df).fit(cov_type='HC3')\n",
    "\n",
    "# Tabela de resultados (apenas OLS)\n",
    "print(\"\\n📋 Tabela de regressão OLS (H3a/b):\")\n",
    "tabela_ols = modelo_ols.summary2().tables[1].reset_index()\n",
    "tabela_ols['Variável'] = tabela_ols['index'].apply(limpar_nome_variavel_com_descricao)\n",
    "print(\"🔍 Colunas em tabela_ols:\", tabela_ols.columns.tolist())\n",
    "p_value_col_ols = 'P>|t|' if 'P>|t|' in tabela_ols.columns else 'P>|z|' if 'P>|z|' in tabela_ols.columns else 'pvalue'\n",
    "stat_col_ols = 't' if 't' in tabela_ols.columns else 'z'\n",
    "tabela_ols['Sig'] = tabela_ols[p_value_col_ols].apply(marcar_sig)\n",
    "\n",
    "# Adicionar a categoria de referência (Alta)\n",
    "alta_row = pd.DataFrame({\n",
    "    'index': ['C(Q(\"Classificação de Coerência\"))[T.Alta]'],\n",
    "    'Coef.': [0.0],\n",
    "    'Std.Err.': [0.0],\n",
    "    stat_col_ols: [0.0],\n",
    "    p_value_col_ols: [1.0],\n",
    "    'Sig': [''],\n",
    "    '[0.025': [0.0],\n",
    "    '0.975]': [0.0],\n",
    "    'Variável': ['Alta (coerência ≥ 0,80)']\n",
    "})\n",
    "tabela_ols = pd.concat([alta_row, tabela_ols], ignore_index=True)\n",
    "\n",
    "# Identificar se é variável de controle\n",
    "tabela_ols[['Ordem', 'É Controle']] = tabela_ols['Variável'].apply(lambda x: pd.Series(extrair_ordem_variavel(x)))\n",
    "\n",
    "# Separar variáveis independentes e de controle\n",
    "tabela_independentes = tabela_ols[tabela_ols['É Controle'] == False].sort_values('Ordem')\n",
    "tabela_controle = tabela_ols[tabela_ols['É Controle'] == True].sort_values('Ordem')\n",
    "\n",
    "# Concatenar as partes sem a linha divisória com NaN\n",
    "tabela = pd.concat([tabela_independentes, tabela_controle], ignore_index=True)\n",
    "\n",
    "# Finalizar a tabela\n",
    "tabela = tabela[['Variável', 'Coef.', 'Std.Err.', stat_col_ols, p_value_col_ols, 'Sig', '[0.025', '0.975]']]\n",
    "tabela.columns = ['Variável', 'Coeficiente', 'Erro Padrão', 'z/t', 'p-valor', 'Sig', 'IC 95% inferior', 'IC 95% superior']\n",
    "tabela = tabela.round(4)\n",
    "tabela.to_excel(\"H3ab_Regressao_Comparativa_Resultados.xlsx\", index=False)\n",
    "\n",
    "# Imprimir a tabela no console\n",
    "print(\"Dependente: log(1 + Cited by) (OLS)\")\n",
    "print(tabela.to_string(index=False))\n",
    "\n",
    "# Resumo técnico limpo\n",
    "print(\"\\n📊 Resumo técnico do modelo:\\n\")\n",
    "resumo_ols = modelo_ols.summary().as_text()\n",
    "substituir_variaveis = {\n",
    "    r'C\\(Q\\(\"Classificação de Coerência\"\\)\\)\\[T\\.([^\\]]+)\\]': r'\\1',\n",
    "    r'Q\\(\"Year\"\\)': \"Ano\",\n",
    "    r'Intercept': 'Intercepto',\n",
    "    r'C\\(Q\\(\"Best Quartile\"\\)\\)\\[T\\.([^\\]]+)\\]': r'Quartil \\1'\n",
    "}\n",
    "resumo_ols_limpo = resumo_ols\n",
    "for padrao, nome_limpo in substituir_variaveis.items():\n",
    "    resumo_ols_limpo = re.sub(padrao, nome_limpo, resumo_ols_limpo)\n",
    "resumo_ols_limpo = re.sub(r'Dep\\. Variable:.*\\n', 'Dep. Variable: log(1 + Cited by)\\n', resumo_ols_limpo)\n",
    "resumo_combined = f\"Modelo OLS:\\n{resumo_ols_limpo}\\nSignificance levels: * p<0.05, ** p<0.01, *** p<0.001\"\n",
    "print(resumo_combined)\n",
    "\n",
    "# Estatísticas descritivas por Classificação de Coerência\n",
    "print(\"\\n📊 Gerando estatísticas por Classificação de Coerência...\")\n",
    "agg = df.groupby('Classificação de Coerência')['Cited by'].agg(['count', 'mean', 'std', 'median', 'min', 'max']).round(3)\n",
    "agg.columns = ['N Artigos', 'Média', 'Desvio Padrão', 'Mediana', 'Mínimo', 'Máximo']\n",
    "agg = agg.sort_values('Média', ascending=False)\n",
    "agg.to_excel(\"H3ab_Resumo_Por_Coerencia.xlsx\")\n",
    "\n",
    "# Relatório final\n",
    "fim = datetime.now(SC_TZ)\n",
    "dur = str(fim - inicio).split('.')[0]\n",
    "print(\"\\n📁 Arquivos gerados:\")\n",
    "print(\"  • H3ab_Regressao_Comparativa_Resultados.xlsx\")\n",
    "print(\"  • H3ab_Resumo_Por_Coerencia.xlsx\")\n",
    "print(\"\\n===== RELATÓRIO FINAL =====\")\n",
    "print(f\"Tempo de execução: {dur}\")\n",
    "print(f\"Finalizado em: {fim:%Y-%m-%d %H:%M:%S} (Florianópolis | SC)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da57d7ed-a728-41a2-ab07-237489be56ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
